<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[推荐计算机科学书籍]]></title>
    <url>%2F2018%2F08%2F30%2Fdouban-computer-books%2F</url>
    <content type="text"><![CDATA[值得一读的高分计算机书籍：https://www.douban.com/doulist/37472347/?start=0&amp;sort=seq&amp;playable=0&amp;sub_type= 从我的角度来看呢，高级程序员推荐必读 计算机基础深入理解计算机系统（英文版·第2版） 网络相关HTTP权威指南 图解HTTP TCP/IP详解 卷1：协议 操作系统现代操作系统（原书第4版） Operating Systems 在线免费版本 数据库高性能MySQL 算法算法（第4版） 算法导论（原书第3版） Java]]></content>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch-nested-query]]></title>
    <url>%2F2018%2F08%2F27%2FElasticsearch-nested-query%2F</url>
    <content type="text"><![CDATA[Elasticsearch嵌套查询，具体可参考Nested Query. 建议还是要多看多熟悉Elasticsearch的官方文档，比到处去搜强多了。 简而言之，在对ES doc的多层嵌套对象进行查询的时候，要使用Nested Query，常规查询无效。 1234567891011121314&#123; &quot;query&quot;: &#123; &quot;nested&quot; : &#123; &quot;path&quot; : &quot;obj&quot;, &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123;&quot;obj.info.name&quot; : &quot;zhangsan&quot;&#125; &#125; ] &#125; &#125; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RBAC简要设计]]></title>
    <url>%2F2018%2F06%2F07%2FRBAC-system-design%2F</url>
    <content type="text"><![CDATA[Role-Based-Access-Control System Design.比较常见的基于角色的访问控制系统，这次是主要了解和简单设计，不涉及到特别复杂的功能。 介绍参考文档：https://blog.csdn.net/yangwenxue_admin/article/details/73936803 RBAC（Role-Based Access Control，基于角色的访问控制），就是用户通过角色与权限进行关联。 其主要特点如下： 一个用户拥有若干角色，每一个角色拥有若干权限。这样，就构造成“用户-角色-权限”的授权模型。这种模型中，用户与角色之间，角色与权限之间，一般是多对多的关系。在此基础上，可以扩展出用户组等实体类型。根据实际需求，可以考虑增加用户组，并对具体权限的类型进行了细分。 角色角色是什么？可以理解为一定数量的权限的集合，权限的载体。例如：一个论坛系统，“超级管理员”、“版主”都是角色。版主可管理版内的帖子、可管理版内的用户等，这些是权限。要给某个用户授予这些权限，不需要直接将权限授予用户，可将“版主”这个角色赋予该用户。 用户组当用户的数量非常大时，要给系统每个用户逐一授权（授角色），是件非常烦琐的事情。这时，就需要给用户分组，每个用户组内有多个用户。 除了可给用户授权外，还可以给用户组授权。这样一来，用户拥有的所有权限，就是用户个人拥有的权限与该用户所在用户组拥有的权限之和。 权限类型权限细分出“权限类型”，我们根据它的取值来区分是哪一类权限，进而与特定的表进行关联。如“MENU”表示菜单的访问权限、“RESOURCE”表示可供访问的URL、“FILE”表示文件的修改权限等。 这样设计的好处有二： 不需要区分哪些是权限操作，哪些是资源，（实际上，有时候也不好区分，如菜单，把它理解为资源呢还是功能模块权限呢？）。 方便扩展，当系统要对新的内容或实体进行权限控制时，我只需要建立一个新的关联表“权限XX关联表”，并确定这类权限的权限类型字符串。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-Basic]]></title>
    <url>%2F2018%2F01%2F25%2FDocker-Basic%2F</url>
    <content type="text"><![CDATA[鉴于生产环境的上线部署，都依赖于镜像构建、制作、部署上线运行等操作，作为一名合格的RD，当然不能只局限于在上线平台上进行一顿鼠标操作了，一定要弄懂这些基础设施内部到底在干嘛。因此，对Docker的相关学习也是很有必要的。 基础信息http://dockone.io/article/783 http://merrigrove.blogspot.com/2015/10/visualizing-docker-containers-and-images.html 两篇文章分别是中文和英文原版，建议初学者多读几遍，收获非常大。尤其是对镜像只读层和读写层的理解，非常重要，还有docker各个命令对各层的影响。 Docker runhttps://docs.docker.com/engine/reference/commandline/run/ docker run [OPTIONS] IMAGE [COMMAND] [ARG…] 常用选项： –detach , -d Run container in background and print container ID –tty , -t Allocate a pseudo-TTY –interactive , -i Keep STDIN open even if not attached –publish , -p Publish a container’s port(s) to the host –volume , -v Bind mount a volume 123456789// sshd -D 将 sshd 作为前台进程运行，而不是脱离控制台成为后台守护进程。主要用于调试。// -it 通常一起使用，可以开启一个终端进去交互模式，调试时很有用。docker run -d -p 2222:22 tomcat:centos /usr/sbin/sshd -Ddocker run -ti -v ~/Downloads:/Downloads tomcat:centos /bin/bashdocker run -d -p 8000:8080 -p 1098:1099 tomcat:centos /usr/local/sbin/tomcat.shdocker run -it -p 8000:8080 -p 1098:1099 tomcat:centos /usr/local/sbin/tomcat.sh 其他Docker命令概览 docker version docker info docker stop $(docker ps -aq) docker rm $(docker ps -aq) docker pull docker login docerk rmi docker images 镜像类1234567891011121314# 检索image$docker search image_name# 下载image$docker pull image_name# 列出镜像列表; -a, --all=false Show all images; --no-trunc=false Don&apos;t truncate output; -q, --quiet=false Only show numeric IDs$docker images# 删除一个或者多个镜像; -f, --force=false Force; --no-prune=false Do not delete untagged parents$docker rmi image_name# 显示一个镜像的历史; --no-trunc=false Don&apos;t truncate output; -q, --quiet=false Only show numeric IDs$docker history image_name 容器类123456789101112131415161718192021222324252627282930313233343536373839# 列出当前所有正在运行的container$docker ps# 列出所有的container$docker ps -a# 列出最近一次启动的container$docker ps -l# 保存对容器的修改; -a, --author=&quot;&quot; Author; -m, --message=&quot;&quot; Commit message $docker commit ID new_image_name# 删除所有容器$docker rm `docker ps -a -q` # 删除单个容器; -f, --force=false; -l, --link=false Remove the specified link and not the underlying container; -v, --volumes=false Remove the volumes associated to the container$docker rm Name/ID# 停止、启动、杀死一个容器$docker stop Name/ID$docker start Name/ID$docker kill Name/ID# 从一个容器中取日志; -f, --follow=false Follow log output; -t, --timestamps=false Show timestamps$docker logs Name/ID # 列出一个容器里面被改变的文件或者目录，list列表会显示出三种事件，A 增加的，D 删除的，C 被改变的$docker diff Name/ID # 显示一个运行的容器里面的进程信息$docker top Name/ID# 从容器里面拷贝文件/目录到本地一个路径 $docker cp Name:/container_path to_path$docker cp ID:/container_path to_path# 重启一个正在运行的容器; -t, --time=10 Number of seconds to try to stop for before killing the container, Default=10$docker restart Name/ID# 附加到一个运行的容器上面; --no-stdin=false Do not attach stdin; --sig-proxy=true Proxify all received signal to the process $docker attach ID Dockerfileto be continued]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Basic Usage]]></title>
    <url>%2F2016%2F05%2F18%2FHexo-Basic%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[工作一年的技术成长总结]]></title>
    <url>%2F2016%2F04%2F20%2Ftech-growup-record%2F</url>
    <content type="text"><![CDATA[工作一年多以来，学了不少东西，也尝试了许多新东西，也成长了不少，也许只是一个粗略的记录，也希望提醒自己，还是取得了一些成绩吧，但不能骄傲，还有许多不足之处，技术永无止境，还需要更多的努力。 工作初始，接触的是C语言网络编程和后台编程的项目，承担的是整体系统中的一部分，加上之前对Redis的了解，也比较适合在这里使用，在设计的时候也就用上了。因为的自己负责调研的部分，所以许多都可以自己设计，还是很高兴的，在使用Redis的过程中，也简单分析了Redis的代码，以及在Linux下编程的技巧，包括学习Redis的Makefile，对错误和消息的处理等。使用Redis作为缓存队列，也实现了自己想要的效果，当然这里会有更优的解决方案，但是作为自己在项目中的第一个设计，还是基本满意的。 再后来的项目，大部分的经历就转到Java语言项目和Java Web方面了，也接触了一些项目的框架，修改功能代码等。独当一面的是另外一个基于大数据的演示性项目，将来会把这个搭建在服务器上。这个项目里，我独立完成了项目的Java Web后台设计，使用了包括Spring，Struts，MongoDB，Morphia等技术，独立设计前台HTML和JS等，使用了Bootstrap，Echarts，JQuery，Semantic-ui等。项目的时间比较久，后来还有一些改进方案没来得及实施，但是从我的角度来说，是我的一个大作品。]]></content>
      <categories>
        <category>Diary</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Gradle使用代理服务器]]></title>
    <url>%2F2016%2F03%2F23%2Fgradle%2F</url>
    <content type="text"><![CDATA[在国内使用Gradle的时候，由于依赖管理时经常需要从mavenCentral（maven中央仓库）和jCenter中下载lib，速度不稳定，有时还会导致build长时间卡住，有一种方法是使用Gradle的Offline模式，但前提是你已经cache了项目的依赖在本地，不然可能会Build失败，另外就是使用代理服务器，也是一种不错的选择。 在Gradle中使用代理服务器的方法： 使用以下命令行参数指定代理服务器。gradle -Dhttp.proxyHost=yourProxy -Dhttp.proxyPort=yourPort -Dhttp.proxyUser=usernameProxy -Dhttp.proxyPassword=yourPassoword 修改Gradle用户配置文件。可以在GRADLE_USER_HOME下新建文件gradle.properties，然后设置代理。GRADLE_USER_HOME的路径一般如下:/home/&lt;username&gt;/.gradle/ (Linux)/Users/&lt;username&gt;/.gradle/ (Mac)C:\Users\&lt;username&gt;.gradle\ (Windows)# Http ProxysystemProp.http.proxyHost=www.somehost.orgsystemProp.http.proxyPort=8080systemProp.http.proxyUser=useridsystemProp.http.proxyPassword=passwordsystemProp.http.nonProxyHosts=*.nonproxyrepos.com|localhost Https ProxysystemProp.https.proxyHost=www.somehost.orgsystemProp.https.proxyPort=8080systemProp.https.proxyUser=useridsystemProp.https.proxyPassword=passwordsystemProp.https.nonProxyHosts=*.nonproxyrepos.com|localhost参考： https://docs.gradle.org/current/userguide/build_environment.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Gradle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bug Tracker]]></title>
    <url>%2F2016%2F03%2F22%2Fbug-tracker%2F</url>
    <content type="text"><![CDATA[最近在尝试使用一些项目管理工具，之前的文章里提到的代码质量分析工具SonarQube，比较符合使用的期望，效果也不错，还有比较早使用的持续集成工具Jenkins， 然后是缺陷跟踪工具，Bug Tracker System，比较常用的有： Redmine http://www.redmine.org/ 网站本身就是Redmine示例。 Mantis http://www.mantisbt.org/ demo：http://www.mantisbt.org/bugs/my_view_page.php。 Atlassian JIRA https://www.atlassian.com/software/jira/ 应用广泛，有云服务版。 Bugzilla https://www.bugzilla.org/类似的工具还有许多，可以查看wiki百科汇总 https://en.wikipedia.org/wiki/Comparison_of_issue-tracking_systems。]]></content>
      <categories>
        <category>开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL database character set encoding]]></title>
    <url>%2F2016%2F03%2F08%2Fmysql-database-character-set-encoding%2F</url>
    <content type="text"><![CDATA[使用MySQL的时候很可能遇到字符集编码相关的问题，尤其是涉及到数据和程序中有中文字符的时候，如果不注意，可能遇到乱码或一些其他的错误。 本文详细解释MySQL相关的字符集编码设置和排序规则相关的问题。 MySQL Server的默认字符集配置在默认安装MySQL的时候，MySQL Server使用的是英文字符集，服务端的默认配置一般是 character-set-server=latin1 collation-server =latin1_swedish_ci 注意latin1字符集是不支持中文的。第一行的character-set当然是指字符集，第二行的collation是指对应该字符集的比较和排序规则。 charset-server参考手册 通过MySQL命令 mysql&gt; SHOW VARIABLES LIKE ‘character%’;可以查看当前服务端的默认配置。 如果在新建数据库的时候不指定character-set和collation，那么就会采用以上的服务器端默认值，所以还是推荐大家手动指定。示例如下： CREATE DATABASE mydb DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;使用utf8和utf8_general_ci是在中英文应用环境下比较常用的一种设置，排序规则还有utf8_unicode_ci，另外还有编码utf8mb4和对应的排序规则，具体区别会在后面的文章说明。 修改已有数据库的字符编码如果之前已经建立好了数据库，需要修改当前数据库的编码，可以使用ALTER DATABASE命令。 首先查看当前数据库的编码和排序规则； mysql&gt; USE mydb;Database changedmysql&gt; SHOW VARIABLES LIKE ‘character_set_database’;+————————+——-+| Variable_name | Value |+————————+——-+| character_set_database | utf8 |+————————+——-+1 row in set (0.00 sec) mysql&gt; mysql&gt; SHOW VARIABLES LIKE ‘collation_database’;+——————–+—————–+| Variable_name | Value |+——————–+—————–+| collation_database | utf8_general_ci |+——————–+—————–+1 row in set (0.01 sec) mysql&gt;然后就可以根据情况修改为自己需要的编码设置了； mysql&gt; ALTER DATABASE databasename CHARACTER SET utf8 COLLATE utf8_general_ci;参考： 数据库的字符集 修改具体table编码的方法，charset-unicode-upgrading参考手册。 修改MySQL的服务端配置修改my.cnf配置文件可以修改MySQL Server的默认字符集等设置。以配置文件在/etc/my.cnf（可能根据具体安装情况不同）为例，修改以下几项即可： [client]default-character-set = utf8 [mysql]default-character-set = utf8 [mysqld]init-connect = ‘SET NAMES utf8’character-set-server = utf8collation-server = utf8_unicode_ci参考 配置方法：http://stackoverflow.com/questions/3513773/change-mysql-default-character-set-to-utf-8-in-my-cnf SET NAMES的官方解释&nbsp;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gradle与SonarQube的应用]]></title>
    <url>%2F2016%2F03%2F04%2Fgradle-sonarqube%2F</url>
    <content type="text"><![CDATA[首先是官方文档。这里使用的是新的Gradle SonarQube plugin，注意与以往的Gradle Sonar和Runner插件区分，官方不推荐使用旧插件。 SonarQube插件说明 Github示例可以参考java-gradle-simple，注意里面build.gradle脚本的写法，以及如何执行SonarQube的Task。 &nbsp;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Gradle</tag>
        <tag>SonarQube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本站开始启用全站Https]]></title>
    <url>%2F2016%2F03%2F04%2Fwebsite-https%2F</url>
    <content type="text"><![CDATA[从今天起本Blog开始启用全站Https。未来的大趋势嘛，哈哈！]]></content>
      <categories>
        <category>网站</category>
      </categories>
      <tags>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins SonarQube的搭配使用]]></title>
    <url>%2F2016%2F03%2F04%2Fjenkins-sonarqube%2F</url>
    <content type="text"><![CDATA[Jenkins在Ubuntu环境下的安装配置都比较简单，在安装好Java JDK之后，使用 sudo apt-get install jenkins即可安装。因为没有用到后台数据库，配置过程一般就是配置端口号，以及Nginx或Apache server的代理即可。 详细方法可以参考官方安装指南。 接下来就是与代码质量分析平台SonarQube的结合使用，前面已经说明了SonarQube的安装，然后就是利用Jenkins在进行持续集成的过程中，进行代码质量分析、代码覆盖率分析，并将相关数据和报告通知给SonarQube。 在Jenkins中的“系统管理”-“管理插件”中搜索安装SonarQube Plugin，因为我使用的是Java Gradle工程和JaCoCo测试报告，所以之前也安装了Gradle Plugin和JaCoCo Plugin，这里大家可以根据自己具体的项目选择。 在安装好SonarQube Plugin之后，记得在系统设置中配置SonarQube服务器的相关信息，可以参考下图进行。 另外注意配置SonarQube scanner，这里可以选择自动安装，或者选择自己安装的目录位置。 服务器配置好之后，然后就是在具体的项目中配置构建过程，选择“增加构建步骤”中的Invoke Standalone SonarQube Analysis，参考下图。 具体的配置如下： # required metadatasonar.projectKey=pminer:MongoDB-ImportXMLProfilesonar.projectName=MongoDB-ImportXMLProfilesonar.projectVersion=1.0 path to source directories (required)sonar.sources=src/main/java path to test source directories (optional)sonar.tests=src/test/java sonar.java.binaries=build/classes sonar.language=java #Tells SonarQube where the unit tests execution reports aresonar.junit.reportsPath=reports/tests #Tells SonarQube where the unit tests code coverage report issonar.jacoco.reportPath=build/jacoco/test.exec Encoding of the source filessonar.sourceEncoding=UTF-8注意以上的配置要根据自己具体的项目路径配置。 这样在下次的构建中，就会之前SonarQube的分析任务，并将结果发送给SonarQube服务器，然后访问服务器平台就能看到代码的质量报告。 参考：http://docs.sonarqube.org/display/PLUG/Code+Coverage+by+Unit+Tests+for+Java+Project]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SonarQube</tag>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SonarQube-代码质量管理平台的安装]]></title>
    <url>%2F2016%2F02%2F25%2Fsonarqube-install%2F</url>
    <content type="text"><![CDATA[安装依赖JDK，数据库（以下以MySQL为例），操作系统支持Linux和Windows（文章以Linux Ubuntu 14.04为例）。 数据库配置终端进入mysql-client： mysql -u root -p 执行以下SQL语句建立数据库和相关用户： CREATE DATABASE sonar CHARACTER SET utf8 COLLATE utf8_general_ci; CREATE USER 'sonar' IDENTIFIED BY 'sonar'; GRANT ALL ON sonar.* TO 'sonar'@'%' IDENTIFIED BY 'sonar'; GRANT ALL ON sonar.* TO 'sonar'@'localhost' IDENTIFIED BY 'sonar'; FLUSH PRIVILEGES; ## 下载并解压SonarQube安装包 在[SonarQube官网](http://www.sonarqube.org/downloads/)获取最新的下载地址。 wget https://sonarsource.bintray.com/Distribution/sonarqube/sonarqube-5.3.zip unzip sonarqube-5.3.zip sudo mv sonarqube-5.3 /usr/local/sonar 编辑配置文件sonar.properties编辑conf目录下的sonar.properties，主要修改数据库配置和web server配置，取消相应行的注释并编辑为对应的值。 sonar.jdbc.username=sonar sonar.jdbc.password=sonar sonar.jdbc.url=jdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;characterEncoding=utf8&amp;rewriteBatchedStatements=true&amp;useConfigs=maxPerformance 以下的web server配置允许以下地址访问 http://127.0.0.1:9000/sonar sonar.web.host=127.0.0.1 #默认是0.0.0.0，绑定本机所有ip地址 sonar.web.context=/sonar #默认是空 sonar.web.port=9000 配置Service运行参考官方文档 新建/etc/init.d/sonar文件并编辑如下。 #!/bin/sh# rc file for SonarQube# chkconfig: 345 96 10description: SonarQube system (www.sonarsource.org)# BEGIN INIT INFOProvides: sonarRequired-Start: $networkRequired-Stop: $networkDefault-Start: 3 4 5Default-Stop: 0 1 2 6Short-Description: SonarQube system (www.sonarsource.org)Description: SonarQube system (www.sonarsource.org)END INIT INFO/usr/bin/sonar $*运行以下命令安装服务并运行，注意bin子目录的32位64位区别。 sudo ln -s $SONAR_HOME/bin/linux-x86-64/sonar.sh /usr/bin/sonarsudo chmod 755 /etc/init.d/sonarsudo update-rc.d sonar defaultssudo service sonar start&nbsp;]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SonarQube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu14.04使用ppa源安装PHP-7]]></title>
    <url>%2F2016%2F02%2F05%2Fubuntu-ppa-install-php7%2F</url>
    <content type="text"><![CDATA[在Ubuntu 14.04下安装PHP除了可以直接从官网下载源码编译安装，也可以PPA源安装。如果读者对编译安装的各种选项和配置方法不是很熟悉的话，则推荐使用这种方法快速安装。 在安装的时候，这里选择的是比较流行的一位个人作者维护的一个PPA源，具体的使用方法如下： 添加源。 sudo LC_ALL=en_US.UTF-8 add-apt-repository ppa:ondrej/php 如果有之前使用apt-get方法安装的PHP，先删除后再安装PHP7。 sudo apt-get update sudo apt-get purge php5-common -y sudo apt-get install php7.0 php7.0-fpm php7.0-mysql -y sudo apt-get --purge autoremove -y 如果使用nginx，注意以下配置和相应的用户权限。 fastcgi_pass unix:/var/run/php/php7.0-fpm.sock; 参考： How To Upgrade to PHP 7 on Ubuntu 14.04 How to install PHP 7?]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Ubuntu</tag>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB--mongoimport loses connection when importing big files]]></title>
    <url>%2F2016%2F01%2F20%2Fmongoimport-loses-connection-when-importing-big-files%2F</url>
    <content type="text"><![CDATA[今天在新版本下进行MongoDB数据导入的时候，遇到了这样一个问题，例如使用如下命令： mongoimport -d test -c profile users.dat在之前使用过几次这样的导入方法都没有问题，但这次却提示： 2016-01-20T10:05:25.228+0100 connected to: localhost2016-01-20T10:05:25.735+0100 error inserting documents: lost connection to server2016-01-20T10:05:25.735+0100 Failed: lost connection to server2016-01-20T10:05:25.735+0100 imported 0 documents查看MongoDB的Log，发现出现异常的原因，如下： 2016-01-20T11:26:39.103+0800 I - [conn17] Assertion: 10334:BSONObj size: 33562755 (0x2002083) is invalid. Size must be between 0 and 16793600(16MB) First element: insert: “Profile”搜索解决方案，发现这是mongo工具包在新版本下的小bug，mongorestore和mongoimport都有一样的问题，官方说明可以参考https://jira.mongodb.org/browse/TOOLS-939。 原因就是bulk write api，原来的api中批量写入的batch size最大是32MB，现在已经变为16MB了。在导入或还原数据的时候，指定选项 –batchSize=1000，指定一个较小的值即可，默认是10000。 参考： http://stackoverflow.com/questions/33475505/mongodb-mongoimport-loses-connection-when-importing-big-files https://jira.mongodb.org/browse/TOOLS-939 http://chenzhou123520.iteye.com/blog/1641319]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu, Debian中iptables规则保存和重启自动加载]]></title>
    <url>%2F2016%2F01%2F13%2Fdebian-iptables-save%2F</url>
    <content type="text"><![CDATA[在Debian中iptables命令输完之后会立刻生效，但重启之后配置就会消失，Debian提供了一个iptables-save程序快速保存配置。 通过iptables-save和iptables-restore可以让debian自动保存并在开机时自动加载iptables规则。 1、将iptables配置保存到/etc/iptables，这个文件名可以自己定义，与下面的配置一致即可iptables-save &gt; /etc/iptables 2、创建并编辑自启动配置文件，内容为启动网络时恢复iptables配置sudo vim /etc/network/if-pre-up.d/iptables文件内容为： #!/bin/sh/sbin/iptables-restore &lt; /etc/iptables保存并退出。 之后系统每次启动时iptables就可以自动加载规则了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Debian</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ相关产品注册]]></title>
    <url>%2F2016%2F01%2F04%2Fintellij-products-register%2F</url>
    <content type="text"><![CDATA[需要的同学可以查看以下网址。当然还是推荐大家去官网购买使用。 http://idea.lanyus.com/]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>develop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL FIND_IN_SET Usage]]></title>
    <url>%2F2016%2F01%2F04%2Fmysql-find-in-set-usage%2F</url>
    <content type="text"><![CDATA[MySQL手册中FIND_IN_SET函数的语法: FIND_IN_SET(str, strlist) Returns a value in the range of 1 to N if the string str is in the string list strlist consisting of N substrings. A string list is a string composed of substrings separated by “,” characters. If the first argument is a constant string and the second is a column of type SET, the FIND_IN_SET() function is optimized to use bit arithmetic. Returns 0 if str is not in strlist or if strlist is the empty string. Returns NULL if either argument is NULL. This function does not work properly if the first argument contains a comma (“,”) character. 假如字符串str 在由N 子链组成的字符串列表strlist 中，则返回值的范围在 1 到 N 之间。一个字符串列表就是一个由一些被‘,’符号分开的子链组成的字符串。如果第一个参数是一个常数字符串，而第二个是type SET列，则 FIND_IN_SET() 函数被优化，使用比特计算。如果str不在strlist 或strlist 为空字符串，则返回值为 0 。如任意一个参数为NULL，则返回值为 NULL。这个函数在第一个参数包含一个逗号(‘,’)时将无法正常运行。 例如： mysql&gt; SELECT FIND_IN_SET(‘b’,’a,b,c,d’); -&gt; 2SUBSTRING_INDEX(_&lt;code&gt;str,delim,count_) Returns the substring from string str before count occurrences of the delimiter delim. If count is positive, everything to the left of the final delimiter (counting from the left) is returned. If count is negative, everything to the right of the final delimiter (counting from the right) is returned. SUBSTRING_INDEX() performs a case-sensitive match when searching for delim. mysql&gt; SELECT SUBSTRING_INDEX(‘www.mysql.com’, ‘.’, 2); -&gt; ‘www.mysql’mysql&gt; SELECT SUBSTRING_INDEX(‘www.mysql.com’, ‘.’, -2); -&gt; ‘mysql.com’This function is multibyte safe. &nbsp; MySQL中使用WHERE IN进行条件查询的时候，一般情况下，查询的结果和IN中值的顺序并不一致。 有两种方式可以对IN查询的结果进行排序。一种是ORDER BY FIND_IN_SET，另外一种是ORDER BY SUBSTRING_INDEX。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10-kms-openwrt]]></title>
    <url>%2F2015%2F10%2F03%2Fwin10-kms-openwrt%2F</url>
    <content type="text"><![CDATA[http://www.right.com.cn/forum/thread-174651-1-2.html http://www.right.com.cn/forum/thread-174287-1-1.html http://www.right.com.cn/forum/thread-159625-1-1.html openwrt luci 具体安装位置如下： /etc/vlmcsd.ini/usr/sbin/vlmcsdvlmcsd-svn812-2015-08-30-Hotbird64 Password: 2015 具体激活使用方法： slmgr.vbs /skms 192.168.1.1slmgr.vbs /ipk XXXXX-XXXXX-XXXXX-XXXXX-XXXXXslmgr.vbs /atoslmgr.vbs /xpr cd C:\Program Files\Microsoft Office\Office15cscript ospp.vbs /sethst:192.168.1.1cscript ospp.vbs /actcscript ospp.vbs /dstatus 更新下载地址，国内地址：http://download.3kkk.org:90/os/vlmcsd-svn812-2015-08-30-Hotbird64.7z官方地址：http://rghost.net/6G8wYxwnX官方站点：http://forums.mydigitallife.info … n-Windows-platforms解压密码 2015 压缩包指纹：MD50f1a9489ccad4a77d3b8f3ee893ede91SHA1f4daa5cb5f3c4f170c0d343982ce2c1a6a88376f支持windows 10 激活 ，office 2016亲测 可用。说明:现在下载到的一般是零售版，是不能用kms激活的，需要转换成批量授权的才行。附转换工具，就是替换几个文件，下载地址：点我下载 下载附件把附件的三个文件传到路由器上创建目录mkdir /opt/kms （随意自己定位置。位置不限）复制三个文件 vlmcsd kmsserver.pid kmsserver.ini 到 /opt/kms给vlmcsd 添加执行权限 chmod a+x vlmcsd启动kms服务/opt/kms/vlmcsd -i /opt/kms/kmsserver.ini -p /opt/kms/kmsserver.pid测试服务器在window 下进入cmd 切换到kmsclient.exe 目录运行kmsclient 1688 192.168.1.1 Windowskmsclient 1688 192.168.1.1 Office2010kmsclient 1688 192.168.1.1 Office2013分别测试 类似这样就是成功了.KMS Client Emulator started successfully.Successfully received response from KMS Server.KMS Server PID: 55041-00096-200-625305-03-1049-7601.0000-057201.Activation request (KMS V4.0) 1 of 1 sent. &nbsp; 附件下载 路由器kms.rar (201.85 KB, 下载次数: 505) 补充说明一下：官方下载下来是一个5M多的压缩包，里面包含了几乎所有平台的以及源代码binaries 目录里面是编译好的二进制软件，可以直接运行的，针对不同系统分了几个目录 ，这几个目录很明了不用解释了。路由器系统 包括 ddwrt openwrt tomato 等都是基于Linux的系统选择Linux目录。这里有根据cpu不同分了几个目录arm 有一部分路由器是目前相对少一些Intel 很熟了 x86的一般很少有成品的路由器 ，很多都是软路由，mips 绝大数的路由器都是mips 的 bcm artheros的基本都是ppc sparc32 这个很少见了，不是主流的，一些特殊设备进入mips 目录又有2个目录 big-endian little-endian俗称 大端 小端 ，意思是一个数据在内存地址中按什么样的顺序存储大体意思小端 高位 存在高地址 低位存在低地址 大端 高位存低地址 低位存高地址 详情百度不同cpu 系统 使用的方式不一样，我知道的不全不一定准确至少 mips 的ar的cpu是大端的 big-endian 。选择big-endian我们常用的X86结构是小端模式，而KEIL C51则为大端模式。很多的ARM，DSP都为小端模式。有些ARM处理器还可以由硬件来选择是大端模式还是小端模式。大小端序 还和系统有关 具体情况具体分析big-endian目录里面有分几个目录这里是根据使用的c语言运行库来区分的x86的Linux系统 一般都是用的glibc 这个库嵌入式的Linux 用的是uclibc 这个库static 是静态的意思，这里软件不依赖共享的运行库 自己用的自己带了。但是体积大了。openwrt 之类都是用的uclibc这个库进入uclibc这个目录 就是软件了ar71xx 91xx的就选择vlmcsd-mips32r2-openwrt-atheros-ar7xxx-ar9xxx-uclibc这个 vlmcs 是客户端测试 vlmcs − a client for testing and/or charging KMS serversvlmcsd 是一个完整的kms激活服务器 vlmcsd - a fully Microsoft compatible KMS servervlmcsd 包含上面2个的功能 vlmcsdmulti - a multi-call binary containing vlmcs(1) and vlmcsd(8)只用来做激活服务器选用vlmcsd就可以了。 激活方法及命令：Windows 激活命令： CD “%SystemRoot%\SYSTEM32″ CSCRIPT /NOLOGO SLMGR.VBS /SKMS 192.168.0.xxx CSCRIPT /NOLOGO SLMGR.VBS /ATO CSCRIPT /NOLOGO SLMGR.VBS /XPROffice/Project/Visio 2013(2010换下安装路径) 激活命令： 32位：CD “%ProgramFiles(x86)%\MICROSOFT OFFICE\OFFICE15″ 64位：CD “%ProgramFiles%\MICROSOFT OFFICE\OFFICE15″ CSCRIPT OSPP.VBS /SETHST:192.168.0.xxx CSCRIPT OSPP.VBS /ACT CSCRIPT OSPP.VBS /DSTATUS 注意：本帖的目的是在你已经搭建私有kms激活服务器的情况下，使局域网内电脑可以自动发现kms服务器而进行免配置激活的。 应用前提是你已经搭好了KMS服务器！ 在openwrt上搭建KMS： http://www.right.com.cn/forum/thread-174287-1-1.html 在cubieboard、树莓派等ARM盒子搭建py-KMS的教程： http://www.cnblogs.com/bitspace/ 结合 @Vincent-Emiya 的测试发现，可以使用DNS指向任意公共的KMS激活服务器实现激活局域网内的主机。这可能是有史以来最便捷的KMS激活方案了。 想象下，只要配置好路由器的DNS，然后不用架设KMS服务器，不用安装小工具，也不要执行任何命令。只要把电脑接入你的局域网，你的系统和office就可以自动激活~不要方便太多 具体方法20楼：http://www.right.com.cn/forum/fo … =174651&amp;pid=1111580 相信很多人都在自己的局域网内搭建了自己的私有kms激活服务器，比如：http://www.right.com.cn/forum/thread-174287-1-1.html。 可以说py-kms与vlmcsd的适用性真的非常之广，不管你在windows，linux下甚至安卓下都可以搭建私有的kms服务。但是最后都会遇到的问题是需要在被激活主机上运行批处理命令，不免有些繁琐。 曾闻中国某高等学府批量购买企业windows许可，你的电脑只要连入校园网，不需要任何配置就可以激活系统，不免神往。查资料发现，这是通过配置DNS服务器的SRV项实现局域网内主机自动发现kms激活服务器的。 刚好我的路由器跑着openwrt系统，可以配置dnsmasq提供SRV功能，于是ssh进入路由器后台，在/etc/dnsmasq.conf中添加配置： srv-host=_vlmcs.tcp.lan,cubietruck.lan,1688,0,100 复制代码_ 其中 _vlmcs._tcp 为服务名；lan 为我的内网域名(这里要改成你的内网域名，一般都是lan)；cubietruck.lan为我的KMS服务器在内网的地址(这里要改成你的内网KMS服务器地址)；1688为kms激活服务默认端口号；0为优先级；100为权重。 注意需要修改：cubietruck.lan 为你的KMS主机实际所在的地址！ 比如你的KMS服务器架设在路由器上，而路由器的主机名为：openwrt 你的局域网域名后缀为lan（一般都是lan） 那么你的路由器地址为：openwrt.lan其中路由器主机名可以在luci界面的状态页面看到，本地域名后缀可以在dns设置页面看到。 然后在路由器中重启dnsmasq服务 /etc/init.d/dnsmasq restart 切换至windows验证dns配置是否正确，打开命令提示符，运行命令： nslookup -type=srv _vlmcs._tcp.lan 其中 _vlmcs._tcp 表示kms服务类型，lan为我的局域网域名称。 看到返回信息： _vlmcs._tcp.lan SRV service location: priority = 0 weight = 100 port = 1688 svr hostname = cubietruck.lan cubietruck.lan internet address = 192.168.1.126 说明dns配置正确。 这时候看看我自己电脑上的office能不能成功发现kms服务器，还是在管理员权限下运行命令： CD “%ProgramFiles(x86)%\MICROSOFT OFFICE\OFFICE15” CSCRIPT OSPP.VBS /remhst CSCRIPT OSPP.VBS /act CSCRIPT OSPP.VBS /dstatus&nbsp; 其中第一行表示清除之前设置的kms激活服务器地址，第二行手动激活，第三行显示激活状态。最终看到信息 REMAINING GRACE: 180 days (259200 minute(s) before expiring Last 5 characters of installed product key: XTGCT Activation Type Configuration: ALL KMS machine name from DNS: cubietruck.lan:1688 Activation Interval: 120 minutes Renewal Interval: 10080 minutes KMS host caching: Enabled —Exiting—————————–&nbsp; 其中 KMS machine name from DNS: cubietruck.lan:1688 表示能够根据DNS自动发现局域网内的kms激活服务器为cubietruck.lan。 说明office可以完全免配置自动激活。 以后任何电脑只要连接入我的局域网，即可对其VOL版本的office以及windows进行自动激活工作。cool~ 参考： http://blog.14401.cn/post-166.html http://www.cnblogs.com/zhuangxuqiang/archive/2009/04/28/1445113.html https://support.microsoft.com/en-us/kb/816587 http://www.cnblogs.com/bitspace/&nbsp;]]></content>
      <categories>
        <category>网站</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JavaScript的模块化编程]]></title>
    <url>%2F2015%2F08%2F31%2Fjavascript-module%2F</url>
    <content type="text"><![CDATA[http://www.ruanyifeng.com/blog/2 … ule_definition.html为什么讲到JS的模块化编程，也是最近在应用Echarts图标库需要面对的一个问题。可以参考Echarts引入的方法：http://echarts.baidu.com/doc/doc.html#引入ECharts在之前的开发中，为了使用简便，直接采用第三种引入单文件的方式，比较简单，&lt;script src=”js/echarts-all.js”&gt;&lt;/script&gt;就可以了，这样就引入了Echarts所有的图标和地图数据，这个文件是900KB，也就是说，无论你在页面中使用几种图表，都需要加载接近1MB大小的JS库文件，效率还是比较低下的。比较推荐的方法是参考JS的模块化编程和AMD规范，当然使用模块化编程主要不是为了解决加载过多的问题，对于代码的规范和质量都有比较大的帮助。Echarts本身使用了require.js，在前台开发中也可以尝试自己使用require.js。简单介绍：http://www.ruanyifeng.com/blog/2012/11/require_js.html官网：http://requirejs.org/ 建议推荐一个SeaJS 的 CMD 规范，与 AMD 非常类似，在国内的影响力非常大，也非常易于使用，是国人开发的，在 github 上的更新、互动非常频繁。http://seajs.orghttps://github.com/seajs/seajs 两个的区别http://www.zhihu.com/question/20351507/answer/14859415 http://segmentfault.com/a/1190000000733959]]></content>
      <categories>
        <category>网站</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB的数据库复制]]></title>
    <url>%2F2015%2F08%2F31%2Fmongodb-copy-database%2F</url>
    <content type="text"><![CDATA[在MongoDB的客户端可以使用如下命令：db.copyDatabase(“abc”, “abc-tmp”, “localhost”)数据库文件较大的话会需要一段时间。 1，复制数据库，使用copyDatabase命令完成复制数据库，格式:copyDatabase(fromdb,todb,fromhost[,username,password])fromdb：源数据库名称todb：目标数据库名称fromhost：源数据库地址，本地和远程都可以username：远程数据库用户名password：远程数据密码例子：将本地db2库复制本地，并重命名db1&gt; db.copyDatabase(“db2”,”db1”,”localhost”)2，刷新磁盘：将内存中尚未写入磁盘的信息写入磁盘，并锁住对数据库更新的操作，但读操作可以使用，使用runCommand命令,这个命令只能在admin库上执行格式：db.runCommand({fsync:1,async:true})async：是否异步执行lock:1 锁定数据库 3，数据压缩：mongodb的存储结构采用了预分配的机制，长期不断的操作，会留下太多的的碎片，从而导致数据库系统越来越慢。repairDatabase命令是mongodb内置的一个方法，它会扫描数据库中的所有数据，并将通过导入/导出来重新整理数据集合，将碎片清理干净现在看压缩前和压缩后的对比数据，如下所示：PRIMARY&gt; db.t1.storageSize()65232896PRIMARY&gt; db.t1.totalSize()81470432PRIMARY&gt; db.repairDatabase(){ “ok” : 1 }PRIMARY&gt; db.t1.storageSize()65232896PRIMARY&gt; db.t1.totalSize()79851584 4，当然也支持复制Collection，与复制Database类似。 &nbsp;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB的POJO映射]]></title>
    <url>%2F2015%2F08%2F31%2Fmongodb-pojo%2F</url>
    <content type="text"><![CDATA[在MongoDB数据库的Java程序开发过程中，使用了官方支持的POJO映射Morphiahttps://github.com/mongodb/morphia/wiki。 其作用，简而言之，就是完成Java Bean对象与MongoDB数据库中一条记录的映射，大家在Entity中看到的Profile就是这样一个POJO，Profile中嵌套了一些子类，比较容易理解，Morphia的所有映射都可以通过注解来完成，非常简单易用，可以查看上面wiki文档中的说明。另外，Morphia提供了一个基本的DAO类，org.mongodb.morphia.dao.BasicDAO&lt;T,K&gt;详细信息可以查看链接https://rawgit.com/wiki/mongodb/morphia/javadoc/0.108/apidocs/index.html 该类支持基本的CRUD操作，查询也支持各种查询条件，不过在返回结果这边可定制性较弱，通常都是返回整个的POJO类的List，属于粗粒度吧。BasicDAO从根本上都是对mongo-java-driver中一些api的封装，如果需要更全面的功能的话可以选用底层driver，当然这样就是直接跟MongoDB中的BSON打交道了，也就用不上POJO了。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
        <tag>Java</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中对json的处理]]></title>
    <url>%2F2015%2F08%2F31%2Fjava-json%2F</url>
    <content type="text"><![CDATA[Java中有不少处理JSON格式数据的类库，在JSON的官网 http://www.json.org/ 下面对应的各个语言编解码库，Java就有20多种可以使用的类库。 当然，其中最常使用的是json-lib，也是资格比较老的一个库，网上介绍也很多，http://json-lib.sourceforge.net/，但是已经停止更新很久了，最新的版本还是基于JDK1.5的，最后更新日期是2010年的12月。在Struts2中返回JSON格式的数据时，默认使用的就是json-lib。但是总体上来说，现在开发程序还是不建议使用json-lib，毕竟有点太古老了。推荐使用的是号称性能最快的JSON处理器Jackson，社区非常活跃，也一直在更新，功能也很强大，https://github.com/FasterXML/jackson。 可以发现在许多其他Java开源项目中对JSON的处理都在使用Jackson，例如Spring。 有人专门对比了这两个类库的性能，http://wangym.iteye.com/blog/738933测试总结：1、显而易见，无论是哪种形式的转换，Jackson &gt; Gson &gt; Json-lib。Jackson的处理能力甚至高出Json-lib有10倍左右2、JSON-lib似乎已经停止更新，最新的版本也是基于JDK15，而Jackson的社区则较为活跃；3、在测试性能的同时，又以人肉方式对这三个类库转换的正确性 进行了检查 ，三者均达100%正确 ；4、JSON-lib在转换诸如Date类型时较为累赘，如以下是两者的转换结果：JSON-lib：{“brithday”:{“date”:17,”day”:2,”hours”:9,”minutes”:24,”month”:7,”seconds”:26,”time”:1282008266398,”timezoneOffset”:-480,”year”:110}}Jackson：{“brithday”:1282008123101}5、JSON-lib依赖commons系列的包及 ezmorph包共 5个，而Jackson除自身的以外只依赖于commons-logging6、Jackson提供完整基于节点的Tree Model，以及完整的OJM数据绑定功能。 经过几项试验和测试，发现Jackson确实功能很强大，推荐还是默认用Jackson来处理JSON，算是目前Java中比较优秀的JSON处理库。 简单的使用方法：https://github.com/FasterXML/jackson-databind/ 即使是处理比较复杂的JavaBean，嵌套多层并且包含自定义类，以及List等的类Profile，也可以轻松处理，不需要用户过多的配置。当然其他更强大的功能还有待以后发掘。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JSON</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rest api design]]></title>
    <url>%2F2015%2F07%2F18%2Frest-api-design%2F</url>
    <content type="text"><![CDATA[最近在进行一个项目，需要设计一个Server，为移动端的client提供REST风格的服务，RESTful API是现在互联网应用程序中使用比较多、也比较成熟的一种设计理念，以HTTP协议为基础，在项目之前，只是简单听说过REST这种理念，并没有很深入的接触，在api的设计过程中，需要遵循一些原则，但是要注意的是RESTful只是一种风格，并不是标准、协议或者强制性的要求。因此，在设计过程中，我也参考了一些文章和资料，学习如何去设计一套合理规范的API。 在初步的设计中，根据之前的工作经验，采用了比较传统的Spring和Hibernate，根据应用的需求，也采用了许多相关技术，例如Spring-data-jpa和Hibernate Validator等，后面会根据一些具体内容进行总结。 下面地址是一个java轻量级restful框架，里面有不少关于RESTful的介绍和文章链接，推荐阅读。 https://github.com/Dreampie/resty]]></content>
      <categories>
        <category>开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[The measure of a man is what he does with power.]]></title>
    <url>%2F2015%2F07%2F15%2Fthe-measure-of-a-man%2F</url>
    <content type="text"><![CDATA[The measure of a man is what he does with power.]]></content>
      <categories>
        <category>日志</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[12306 AND Apache geode]]></title>
    <url>%2F2015%2F07%2F03%2F12306-and-apache-geode%2F</url>
    <content type="text"><![CDATA[http://geode.incubator.apache.org/ 前段时间听说了分布式内存数据库Apache geode开源的消息，本来对该数据库还不甚了解，但是发现它和12306有着不少关系，不仅被采用而且经受住了重重考验，所以觉得还是有必要关注一下，先留下一篇文章吧，以后来完善。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis传入类型为Long的处理]]></title>
    <url>%2F2015%2F06%2F16%2Fmybatis-parameter-long%2F</url>
    <content type="text"><![CDATA[mybatis中如果传入类型为Long，与其他一些基本类型例如int，String的处理是不一样的，参数需要统一使用#{_parameter}，而不论你传入参数的名称是什么。 例如： &lt;select id=”getUser” parameterType=”java.lang.Long” resultType=”com.test.User”&gt; SELECT * from user &lt;if test = “accountId!=null”&gt; WHERE accountId = #{_parameter} &lt;/if&gt;&lt;/select&gt;&nbsp;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PUT POST PATCH in HTTP]]></title>
    <url>%2F2015%2F06%2F08%2Fput-post-patch-in-http%2F</url>
    <content type="text"><![CDATA[In the HyperText Transfer Protocol (HTTP), idempotence and safety are the major attributes that separate HTTP verbs. Of the major HTTP verbs, GET, PUT, and DELETE are idempotent (if implemented according to the standard), but POST is not.[9] These verbs represent very abstract operations in computer science: GET retrieves a resource; PUT stores content at a resource; and DELETE eliminates a resource. As in the example above, reading data usually has no side effects, so it is idempotent (in fact nullipotent). Storing a given set of content is usually idempotent, as the final value stored remains the same after each execution. And deleting something is generally idempotent, as the end result is always the absence of the thing deleted. 参考 PUT vs POST in REST Idempotence wiki]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>develop</tag>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的psn id]]></title>
    <url>%2F2015%2F05%2F29%2Flmshlms-psn-id%2F</url>
    <content type="text"><![CDATA[[caption id=”” align=”alignnone” width=”480”] 我的psn奖杯卡[/caption] 我的psn id是lmshlms，港服，欢迎大家加好友。]]></content>
      <categories>
        <category>游戏</category>
      </categories>
      <tags>
        <tag>Game</tag>
        <tag>PS4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gradle tomcat相关脚本]]></title>
    <url>%2F2015%2F05%2F29%2Fgradle-tomcat-deploy%2F</url>
    <content type="text"><![CDATA[初用Gradle，也学习了不少东西，相比于之前的maven（虽然用的不多），但还是感受到了Gradle的许多特点和强大之处。除了常规的添加一些依赖，也尝试着自己写一些脚本。下面的跟tomcat相关的用例，主要用于部署测试环境，也借鉴了他人的一些思路，分享出来。 // deploy to local tomcat server // All extra properties must be defined through the “ext” namespace.ext.tomcatHome = System.getenv()[“CATALINA_HOME”]ext.tomcatBin = tomcatHome + ‘/bin’ext.tomcatStart = tomcatBin + ‘/startup’ext.tomcatStop = tomcatBin + ‘/shutdown’ext.tomcatWebapps = tomcatHome + ‘/webapps’ ant.condition(property: “os”, value: “windows”) { os(family: “windows”) }ant.condition(property: “os”, value: “unix” ) { os(family: “unix”) } task checkTomcat &lt;&lt; { if (tomcatHome == null) throw new RuntimeException(“Could not get TOMCAT home, please set CATALINA_HOME env virable first!”) switch(ant.properties.os){ case ‘windows’: println ‘Running on windows.’ tomcatStart += ‘.bat’ tomcatStop += ‘.bat’ break case ‘unix’: println ‘Running on unix.’ tomcatStart += ‘.sh’ tomcatStop += ‘.sh’ break } println “Using CATALINA_HOME: ${tomcatHome}” println “Using Tomcat start cmd: ${tomcatStart}” println “Using Tomcat stop cmd: ${tomcatStop}”} task deployLocal &lt;&lt; { println “copy war from ${buildDir}/libs into ${tomcatWebapps}” copy{ from “${buildDir}/libs” into “${tomcatWebapps}” include ‘*.war’ } //println “start tomcat !” //startTomcat.execute()} deployLocal.dependsOn checkTomcat task startTomcat &lt;&lt; { exec { executable tomcatStart } println ‘Start Tomcat server.’ //store the output instead of printing to the console: standardOutput = new ByteArrayOutputStream() //extension method stopTomcat.output() can be used to obtain the output: ext.output = { return standardOutput.toString() } println &apos;Done.&apos; } startTomcat.dependsOn checkTomcat task stopTomcat &lt;&lt; { exec { executable tomcatStop } println ‘Shutting down Tomcat server.’ //store the output instead of printing to the console: standardOutput = new ByteArrayOutputStream() //extension method stopTomcat.output() can be used to obtain the output: ext.output = { return standardOutput.toString() } println &apos;Done.&apos; } stopTomcat.dependsOn checkTomcat&nbsp;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Gradle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring mvc无法处理put参数的问题]]></title>
    <url>%2F2015%2F05%2F21%2Fspring-mvc-put-params%2F</url>
    <content type="text"><![CDATA[最近在用Spring MVC做一个Restful的Web Service，在HTTP的各类method中，除了非常常用的GET和POST之外，还有PUT、DELETE、PATCH等可以使用。这次遇到问题的是PUT方法，在REST风格中，PUT一般表示对原有资源的修改，但是在CLIENT端编程发出PUT请求后，却总是会遇到400错误，在用另外一个REST测试工具（IDEA中集成）的时候却一切正常。 分析了一下原因，发现几点： 使用的测试工具在发出PUT请求时，参数是带在url尾部的，而不是放在body中，类似于/users/123?name=123&amp;gender=456这样的形式，Spring mvc在处理这种形式的PUT请求时是没有问题的。 在编写Client的过程中，发送HTTP PUT请求的时候，一般会把参数放入Request Body中，以key:value的格式发送，本质上是json string的格式，此时，在Spring mvc中，通过注解@RequestParam和@ModelAttribute都不能得到请求数据，问题的原因不在Spring，即使使用原始Servlet的request.getParameter也是得不到数据的。解决方法主要有两种： Spring mvc中处理参数时，改用注解@RequestBody，这里得到的是请求字符串，需要自己进行转换，得到对应的参数信息，Spring这里也提供了对应的转换方法，处理起来并不复杂。 自己增加一个filte，来专门处理HTTP中的PUT请求，这里建议参考该文章。关键配置部分如下：&lt;filter&gt;&lt;filter-name&gt;httpPutFormFilter&lt;/filter-name&gt;&lt;filter-class&gt;org.springframework.web.filter.HttpPutFormContentFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt;&lt;filter-name&gt;httpPutFormFilter&lt;/filter-name&gt;&lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&nbsp; &nbsp;]]></content>
      <categories>
        <category>Java</category>
        <category>网站</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gradle执行外部命令]]></title>
    <url>%2F2015%2F05%2F19%2Fgradle-exec-command%2F</url>
    <content type="text"><![CDATA[在Gradle的官方网站，学习的话首要看的是User Guide，其次就是DSL Reference，我觉得后者其实更加重要，前者的内容比较基础，示例也比较精简，所以推荐到DSL里去看示例。 例如关于任务类型Copy和Exec的介绍。 task stopTomcat(type:Exec) { workingDir ‘../tomcat/bin’ //on windows: commandLine ‘cmd’, ‘/c’, ‘stop.bat’ //on linux commandLine ‘./stop.sh’ //store the output instead of printing to the console: standardOutput = new ByteArrayOutputStream() //extension method stopTomcat.output() can be used to obtain the output: ext.output = { return standardOutput.toString() }}这个示例就很好的介绍了在Gradle中如何执行外部命令。 &nbsp;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Gradle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[To Liu]]></title>
    <url>%2F2015%2F05%2F19%2Fto-liu%2F</url>
    <content type="text"><![CDATA[Liu Xi:You will never know how many time I met with you in my dream, even like last night.I really want to be together with you, just like back in my childhood. You can’t imagine,when I was really bold enough to say your name in front of the class and our teacher. Iknow what your really name is, of course I know. I can’t just mention your really name anymore. I’m so sorry. Even in my dream, that’s just one day.I ran after you, I know your face,it’s still like 20 years ago. I really wish you could stay with me for a little longer. ButI can’t find you in the end and I was back at my college. Still like the past, if I could choose again, I would choose to stay with you all. It’s notmy choise, and I could not blame my father. And now, I know you are married, you have a happylife. And I know it’s better that way than with me. I wish I could meet you again in my dream. Best WishesFarewell Yours,Lei 2015/5/21 After so long time, I’m back here. And I know, deeply in my heart, it’s you. It’s pure emotion about you. 2016/3/8]]></content>
      <categories>
        <category>日志</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Gradle Check OS]]></title>
    <url>%2F2015%2F05%2F15%2Fgradle-check-os%2F</url>
    <content type="text"><![CDATA[最近使用Gradle来构建一个Spring Project，相比于传统的maven，Gradle确实有不少优点，但由于Gradle是基于Groovy语音，初用起来还是有许多不熟悉的地方，其实在简单的使用时，IDE会帮我们完成build.gradle中大多数的内容，我们也就可以直接完成构建构成了，再加上我的项目使用了Spring-boot，加上Gradle插件后，并不需要自己去写太多的脚本。总之，即使你没接触过maven，Gradle也是很容易使用和上手的。 今天遇到的一个问题，就是在脚本中判断你当前使用的操作系统，因为我这里要设置Tomcat的路径，在windows和Linux之间还是有很大区别的，基本方法如下： 利用ant ant.condition(property: “os”, value: “windows”) { os(family: “windows”) }ant.condition(property: “os”, value: “unix” ) { os(family: “unix”) } task checkOS &lt;&lt; { switch(ant.properties.os){ case ‘windows’: println ‘This is windows.’ break case ‘unix’: println ‘This is unix.’ break }} import org.apache.tools.ant.taskdefs.condition.Ostask checkWin() &lt;&lt; { if (Os.isFamily(Os.FAMILY_WINDOWS)) { println “WINDOWS “ }}利用系统属性 task checkOS &lt;&lt; { if (System.properties[‘os.name’].toLowerCase().contains(‘windows’)) { println “it’s Windows” } else { println “it’s not Windows” }}&nbsp;]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Gradle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查看Linux版本号]]></title>
    <url>%2F2015%2F05%2F11%2Flinux%2F</url>
    <content type="text"><![CDATA[查看内核版本号 cat /proc/versionLinux version 3.2.0-29-generic (buildd@allspice) (gcc version 4.6.3 (Ubuntu/Linaro 4.6.3-1ubuntu5) ) #46-Ubuntu SMP Fri Jul 27 17:03:23 UTC 2012 uname -aLinux 3.2.0-29-generic #46-Ubuntu SMP Fri Jul 27 17:03:23 UTC 2012 x86_64 x86_64 x86_64 GNU/Linux查看Linux发行版的相关信息 lsb_release-aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 14.04.2 LTSRelease: 14.04Codename: trusty cat /etc/issueUbuntu 14.04.2 LTS \n \l &nbsp;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS new Date in firefox]]></title>
    <url>%2F2015%2F04%2F30%2Fjs-new-date-in-firefox%2F</url>
    <content type="text"><![CDATA[JavaScript的new Date()，传入格式化日期参数，在Firefox中会比较严格，例如以下代码 var date = new Date(“2015-4-30”);在Chrome下运行成功，在Firefox下则是“Invalid Date”。 显然以下代码才是标准的格式 var date = new Date(“2015-04-30”);或者 var date = new Date(“2015/4/30”);&nbsp;]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java热部署工具-JRebel]]></title>
    <url>%2F2015%2F04%2F30%2Fjrebel-redeploy%2F</url>
    <content type="text"><![CDATA[之前看了一篇文章，讲5个比较优秀的Java项目，文章虽然比较老，但是对里面的Neo4J，Gradle和JRebel都有了不少了解。 最近开发一个Java Web项目，经常需要部署到Tomcat上进行调试，但是每次修改代码或者配置文件后，都需要重新部署并重启Tomcat容器，实在是很浪费时间的一项操作。于是决定尝试一下JRebel，实现热部署，对程序的修改可以直接反应在部署的程序上。 可惜的是，JRebel是一个收费软件，个人可以申请30天的免费试用，可以使用myJRebel，https://my.jrebel.com/，是JRebel的一个社区计划，会要求你允许分享一些使用信息，在这里可以免费申请一个key，但需要定期联网激活。 JRebel对各种IDE都有相应的插件，使用起来非常方便，具体的信息可以查看http://zeroturnaround.com/software/jrebel/learn/。选择JRebel monitor的项目，并在Tomcat server中配置使用即可。启动Tomcat后可以看到JRebel的信息，例如 2015-04-30 10:26:45 JRebel:2015-04-30 10:26:45 JRebel: #############################################################2015-04-30 10:26:45 JRebel:2015-04-30 10:26:45 JRebel: JRebel Agent 6.1.2 (201504061000)2015-04-30 10:26:45 JRebel: (c) Copyright ZeroTurnaround AS, Estonia, Tartu.2015-04-30 10:26:45 JRebel:2015-04-30 10:26:45 JRebel: Over the last 6 days JRebel prevented2015-04-30 10:26:45 JRebel: at least 166 redeploys/restarts saving you about 6.7 hours.2015-04-30 10:26:45 JRebel:2015-04-30 10:26:45 JRebel: Licensed to Mingshan Lei (using myJRebel).2015-04-30 10:26:45 JRebel:2015-04-30 10:26:45 JRebel:2015-04-30 10:26:45 JRebel: #############################################################2015-04-30 10:26:45 JRebel:使用效果确实非常好，JRebel支持许多框架，常用的Spring、myBatis、Struts等等，可以monitor这些框架的配置文件，修改之后会在控制台显示提示信息，Reload class或者Reload SQL map等，确实可以节约许多用于部署的时间。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[code highlighter test]]></title>
    <url>%2F2015%2F03%2F21%2Fcode-highlighter-test%2F</url>
    <content type="text"><![CDATA[#include &lt;stdio.h&gt; int main(){ printf(“Hello World\n”); return 0;}测试一下这个代码显示插件，Crayon Syntax Highlighter是我在wordpress使用过的最好用方便的代码插件，推荐使用。]]></content>
      <categories>
        <category>网站</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[大端 小端 字节序]]></title>
    <url>%2F2015%2F03%2F20%2Fbig-little-endian%2F</url>
    <content type="text"><![CDATA[#include &lt;stdio.h&gt;/ 关于字节序:对于一个整数0x12346=5678,在内存中占4个字节从低地址开始,每个字节分别存的是 12,34,56,78就说明是大端CPU; 用于网络编程,JAVA从低地址开始,每个字节分别存的是 78,56,34,12就说明是小端CPU; x86架构是小端下面判断在内存的单个字节中二进位是按什么顺序排的? / //定义位域typedef struct bit_field{ unsigned char b0:1; //b0指向低地址那一端的第一位,b1、b2依次排列下去 unsigned char b1:1; unsigned char b2:1; unsigned char b3:1; unsigned char b4:1; unsigned char b5:1; unsigned char b6:1; unsigned char b7:1;}BIT_FIELD; int main(){ int i = 0xFF00001B; //小端存放方式 1B 00 00 FF BIT_FIELD p = ( BIT_FIELD )&amp;i; // p指向 1B printf(&quot;%u &quot;,p-&amp;gt;b0); printf(&quot;%u &quot;,p-&amp;gt;b1); printf(&quot;%u &quot;,p-&amp;gt;b2); printf(&quot;%u &quot;,p-&amp;gt;b3); printf(&quot;%u &quot;,p-&amp;gt;b4); printf(&quot;%u &quot;,p-&amp;gt;b5); printf(&quot;%u &quot;,p-&amp;gt;b6); printf(&quot;%un&quot;,p-&amp;gt;b7); getchar(); return 0; }输出结果：1 1 0 1 1 0 0 0 在小端机器上验证得到, 在一个字节内部, 低位放在靠近低地址这个方向上。大端方式的高位靠近低地址方向存放。 http://blog.csdn.net/ce123_zhouwei/article/details/6971544]]></content>
      <categories>
        <category>开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在VPS上升级Ubuntu12.04到14.04]]></title>
    <url>%2F2014%2F04%2F30%2Fvps-upgrade-ubuntu-12-04-14-04%2F</url>
    <content type="text"><![CDATA[前不久，Ubuntu的长期支持版LTS已经发布了14.04，本站搭建在基于Xen的Linode VPS上，最开始创建系统时选择的是Ubuntu12.04，在VPS中有rebuild的选项可以帮助你重装系统，直接安装最新的Ubuntu14.04，但是数据会丢失，备份和恢复比较费时。其实这里完全可以自己升级，Linode VPS支持我们自己升级内核和整个Linux的系统版本。对于其他的VPS，最好询问一下是否支持这样的升级。 为了数据安全，请在升级前对自己的数据进行完全备份。我这里采用了Linode的Backup服务，基础收费是每月5刀，可以定时或者手动对系统做快照进行备份。 Ubuntu的升级命令很简单，如下所示（需要sudo或root权限）： # apt-get update apt-get upgradedo-release-upgrade -d升级过程中请一定遵照系统的提示，例如系统会新建一个ssh daemon，防止连接中断对升级的影响。基本上按照步骤进行就可以升级成功。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新的工作]]></title>
    <url>%2F2014%2F03%2F30%2Fnew-job%2F</url>
    <content type="text"><![CDATA[之前略有点偷懒，整个个人博客的更新也停止了很长时间。完成了硕士学位论文和毕业阶段，于是就很快进入了新的工作，也要为以后多作计划。 经过一番考虑，这一段准备更新一些之前落下的内容，例如毕设相关的一些技术知识，也准备学习一些新技术，会在这里记录一下自己的收获。OK！今天先到这里吧，继续努力！]]></content>
      <categories>
        <category>日志</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[游戏记录]]></title>
    <url>%2F2013%2F10%2F28%2Fgame-record%2F</url>
    <content type="text"><![CDATA[之前用一个txt记录自己的游戏生涯，不是很方便，想想还是发布在这里吧，许多一线的大作基本都是发布后几天就通关的，像最近的《蝙蝠侠：阿甘起源》，过几天还会有《战地4》和《使命召唤》的年度终极对决。 早些年的记录都没有保存下来，但是像使命召唤系列都是玩了很多遍的，这里的记录也不是很全，也算比较遗憾吧！ 2011.08 使命召唤72011.08.26 堡垒Bastion 2011.10.13 英雄无敌6 简体中文版 战网激活 2011.10.27 战地3 Battlefield3 战役通关2011.10.28 战锤40K：星际战士 战役通关2011.11.09 使命召唤8：现代战争3 单人战役 普通难度通关 4:252011.11.09 使命召唤8：现代战争3 单人战役 困难难度通关 7:502011.11.19 极品飞车16：亡命天涯 剧情模式 普通难度通关2011.11.25 刺客信条：启示录 安装 蝙蝠侠：阿甘之城 激活2011.12.07 蝙蝠侠：阿甘之城 主线通关2011.12.17 Trine 2 魔幻三杰2 通关2012.01.09 《刺客信条.启示录》 主线通关2012.05.05 《PROTOTYPE》 虐杀原型 主线通关2012.05.09 国产游戏 风卷残云 通关2012.05.12 Sniper Elite V2 狙击精英2 开始通关2012.05.142012.05.15 《Diablo 3》 暗黑破坏神3 上线2012.07.06 《马克思佩恩3》 剧情模式 通关2012.07.20 《特殊行动：一线生机》 通关2012.09.03 《变形金刚：塞伯坦的陨落》战役通关2012.09.25 《火炬之光2》 normal难度 通关2012.10.17 《无主之地2》 普通难度 通关2012.10.24 《荣誉勋章：战士》 普通难度 通关2012.11.04 《DeadLight》 普通难度 通关2013.01.23 《The Walking Dead》游戏 1-5章 通关2013.01.25 《鬼泣DMC》2013.01.29 《鬼泣DMC》 Devil Hunter难度通关2013.03.09 《Tomb Raider》 古墓丽影 主剧情 通关2013.03.09 《鬼泣DMC》 DLC Vergil’s Downfall 通关2013.04.11 《蝙蝠侠：阿甘疯人院》 剧情通关2013.05.05 《PROTOTYPE 2》 虐杀原型2 主线 普通难度 通关2013.06.01 《英雄传说：空之轨迹3rd》通关2013.06.06 《Remember Me》勿忘我 主线流程 普通难度通关2013.07.23 《狂野西部 枪手》 Call of Juarez Gunslinger 故事模式 普通难度通关2013.10.28 《蝙蝠侠：阿甘起源》主线通关 2013.11.06 《使命召唤10：幽灵》战役 普通难度通关]]></content>
      <categories>
        <category>游戏</category>
      </categories>
      <tags>
        <tag>Game</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中指针与引用的区别-More Effective C++]]></title>
    <url>%2F2013%2F10%2F07%2Fpointers-references-c%2F</url>
    <content type="text"><![CDATA[指针与引用看上去完全不同（指针用操作符“*”和“-&gt;”，引用使用操作符“. ”），但是它们似乎有相同的功能。指针与引用都是让你间接引用其他对象。你如何决定在什么时候使用指针，在什么时候使用引用呢？ 首先，要认识到在任何情况下都不能使用指向空值的引用。一个引用必须总是指向某些对象。因此如果你使用一个变量并让它指向一个对象，但是该变量在某些时候也可能不指向任何对象，这时你应该把变量声明为指针，因为这样你可以赋空值给该变量。相反，如果变量肯定指向一个对象，例如你的设计不允许变量为空，这时你就可以把变量声明为引用。 “但是，请等一下”，你怀疑地问，“这样的代码会产生什么样的后果？” char *pc = 0; // 设置指针为空值 char&amp; rc = *pc; // 让引用指向空值这是非常有害的，毫无疑问。结果将是不确定的（编译器能产生一些输出，导致任何事情都有可能发生）。应该躲开写出这样代码的人，除非他们同意改正错误。如果你担心这样的代码会出现在你的软件里，那么你最好完全避免使用引用，要不然就去让更优秀的程序员去做。我们以后将忽略一个引用指向空值的可能性。 因为引用肯定会指向一个对象，在C＋＋里，引用应被初始化。 string&amp; rs; // 错误，引用必须被初始化string s(“xyzzy”);string&amp; rs = s; // 正确，rs指向s指针没有这样的限制。 string *ps; // 未初始化的指针// 合法但危险不存在指向空值的引用这个事实意味着使用引用的代码效率比使用指针的要高。因为在使用引用之前不需要测试它的合法性。 void printDouble(const double&amp; rd){ cout &lt;&lt; rd; // 不需要测试rd,它} // 肯定指向一个double值相反，指针则应该总是被测试，防止其为空： void printDouble(const double pd){ if (pd) { // 检查是否为NULL cout &lt;&lt; pd; }}指针与引用的另一个重要的不同是指针可以被重新赋值以指向另一个不同的对象。但是引用则总是指向在初始化时被指定的对象，以后不能改变。 string s1(“Nancy”); string s2(“Clancy”); string&amp; rs = s1; // rs 引用 s1 string *ps = &amp;s1; // ps 指向 s1 rs = s2; // rs 仍旧引用s1, // 但是 s1的值现在是 // “Clancy” ps = &amp;s2; // ps 现在指向 s2; // s1 没有改变总的来说，在以下情况下你应该使用指针，一是你考虑到存在不指向任何对象的可能（在这种情况下，你能够设置指针为空），二是你需要能够在不同的时刻指向不同的对象（在这种情况下，你能改变指针的指向）。如果总是指向一个对象并且一旦指向一个对象后就不会改变指向，那么你应该使用引用。 还有一种情况，就是当你重载某个操作符时，你应该使用引用。最普通的例子是操作符[]。这个操作符典型的用法是返回一个目标对象，其能被赋值。 vector&lt;int&gt; v(10); // 建立整形向量（vector），大小为10; // 向量是一个在标准C库中的一个模板(见条款M35) v[5] = 10; // 这个被赋值的目标对象就是操作符[]返回的值 如果操作符[]返回一个指针，那么后一个语句就得这样写： *v[5] = 10;但是这样会使得v看上去象是一个向量指针。因此你会选择让操作符返回一个引用。（这有一个有趣的例外，参见条款M30） 当你知道你必须指向一个对象并且不想改变其指向时，或者在重载操作符并为防止不必要的语义误解时，你不应该使用指针。而在除此之外的其他情况下，则应使用指针。 &nbsp; 为了进一步加深大家对指针和引用的区别，下面我从编译的角度来阐述它们之间的区别： 程序在编译时分别将指针和引用添加到符号表上，符号表上记录的是变量名及变量所对应地址。指针变量在符号表上对应的地址值为指针变量的地址值，而引用在符号表上对应的地址值为引用对象的地址值。符号表生成后就不会再改，因此指针可以改变其指向的对象（指针变量中的值可以改），而引用对象则不能修改。 最后，总结一下指针和引用的相同点和不同点： 相同点： 都是地址的概念；指针指向一块内存，它的内容是所指内存的地址；而引用则是某块内存的别名。不同点： 指针是一个实体，而引用仅是个别名； 引用只能在定义时被初始化一次，之后不可变；指针可变；引用“从一而终”，指针可以“见异思迁”； 引用没有const，指针有const，const的指针不可变；（具体指没有int&amp; const a这种形式，而const int&amp; a是有的， 前者指引用本身即别名不可以改变，这是当然的，所以不需要这种形式，后者指引用所指的值不可以改变） 引用不能为空，指针可以为空； “sizeof 引用”得到的是所指向的变量(对象)的大小，而“sizeof 指针”得到的是指针本身的大小； 指针和引用的自增(++)运算意义不一样； 引用是类型安全的，而指针不是 (引用比指针多了类型检查)。]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[黑板数字擦除问题-阿里笔试题]]></title>
    <url>%2F2013%2F09%2F22%2Falibaba-contest-problem-blackboard%2F</url>
    <content type="text"><![CDATA[题目： 在黑板上写上1,2,3,4，……，50这50个数字，只要黑板上还有两个或两个以上的数字，就随机选取两个数字a和b，擦除a和b，并写上 |a - b|，问最后黑板上剩下的那个数可能是哪些？ 这道题其实是小学奥数题。。。但是我要说，即将拿到硕士学位的我真的不会做啊。。。。。。 答案是范围内的奇数。解：黑板上所有数的和S＝1+2+3+……+50 是一个奇数，每操作一次，总和S减少了a+b -（a-b）=2b（假设a大于等于b），这是一个偶数，说明总和S的奇偶性不变。由于开始时S是奇数，因此终止时S仍是一个奇数。 参考：http://www.360doc.com/content/11/0312/12/6307578_100433348.shtml]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Google Practice Round China New Grad Test 2014 解题报告]]></title>
    <url>%2F2013%2F09%2F21%2Fgoogle-practice-round-china-new-grad-test-2014%2F</url>
    <content type="text"><![CDATA[这次练习赛是Google针对2014校园招聘即将到来的一个上机测试进行的一次练习，一共三道题目，都不是很难，也主要是可以对系统有一个更好的了解。 先简单说一下Google CodeJam的这个系统，与我们通常的OnlineJudge主要的不同，是提交代码的方式，在我们写好解题代码后，需要下载系统随机提供的测试用例文件，利用自己的程序跑这个输入文件并得到相应的输出，然后上传自己代码和输出给系统去判断，总体来说是复杂了一些。 大家可以通过这个链接https://code.google.com/codejam/contest/2933486/dashboard去查看这次练习的具体题目和测试用例。 Problem A. Bad Horse 题目简单来看，就是一句话：Bad Horse has decided to split the league into two departments in order to separate troublesome members. 每个测试用例呢，会给你几对人的名字，每一对人都是会互相找麻烦的那种，必须分到不同组去，那么最后能不能成功分组呢？ 例如（1,2）（2,3）（3,4）是可以分成（1,3）（2,4）这两个集合的，而（1,2）（2,3）（1,3）则是不可以划分成功的。我对这题使用的方法略复杂，就不贴出代码了，简单说一下基本的思想：利用两个集合A，B来保存划分，对于新出现的一对，查看各个成员在A，B里面的出现情况，保证一个在A，另一个在B，如果两个在之前出现在了同一个集合中，那么结果肯定是No，如果两个都没有出现过，则先不要忙着去划分，先进行后面的，遍历完一趟之后再处理。例如（1,2）（3,4）（1,3），1放在集合A，2放在集合B，你会发现，3和4之前都没有出现过，那怎么放呢，先进行后面的（1,3），走完一趟之后再来处理（3,4）这样的对。 Problem B. Captain Hammer 这其实是一道标准的物理题。一个飞行器，从地面，给你一个斜向上的速度，那么根据角度可以计算出其竖直向上的速度和水平方向的速度，在重力加速度的影响下其最终会落回地面，由竖直向上的速度可以知道时间，那么水平方向速度乘以时间就是其在水平方向的位移。 题目给出初始的斜向速度和最后的水平位移，求斜向角度。 代码如下： #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;math.h&gt; #define PI 3.141592653int main(){ int t; scanf(“%d”, &amp;t); int i; int speed, distance; for (i = 0; i &lt; t; i++) { scanf(“%d %d”, &amp;speed, &amp;distance); double result; result = (double)9.8 distance / speed / speed; result = asin(result) 180.0 / PI / 2; printf(“Case #%d: %.7fn”, i+1, result); } return 0;}Problem C. Moist 这个题目有点插入排序的意思，本身意思很简单，解题思想也很简单，给你一系列的人名，最后要按字典序排序，如果一个人名需要插入到前面去，计数加1，比算移动的次数还简单。 代码如下： #include &lt;iostream&gt; #include &lt;string&gt; using namespace std; int main(){ int t; cin &gt;&gt; t; for (int i = 0; i &lt; t; i++) { int n; cin &gt;&gt; n; cin.get(); int count = 0; string last; getline(cin, last); string current; for (int j = 1; j &amp;lt; n; j++) { getline(cin, current); if (current.compare(last) &amp;lt; 0) count++; else last.assign(current); } cout &amp;lt;&amp;lt;&quot;Case #&quot; &amp;lt;&amp;lt; i+1 &amp;lt;&amp;lt; &quot;: &quot; &amp;lt;&amp;lt; count &amp;lt;&amp;lt; endl; } return 0; }&nbsp;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WINAPI QueryPerformanceCounter的使用]]></title>
    <url>%2F2013%2F09%2F20%2Fhow-to-use-queryperformancecounter%2F</url>
    <content type="text"><![CDATA[引用自http://stackoverflow.com/questions/1739259/how-to-use-queryperformancecounter 最近在做一个程序，要从Linux转移到windows下测试一下计算时间，我们知道在Linux下可以使用gettimeofday得到微妙级别的时间，使用方法先复习一下： 使用到的数据结构和函数原型如下： struct timeval { time_t tv_sec; / seconds / suseconds_t tv_usec; / microseconds /}; #include&lt;sys/time.h&gt;int gettimeofday(struct timeval tv, struct timezone tz);在gettimeofday()函数中tv或者tz都可以为空。如果为空则就不返回其对应的结构体。在struct timeval中的tv_usec域我们可以得到微妙级别的时间。 函数执行成功后返回0，失败后返回-1，错误代码存于errno中。 接下来转入到Windows部分，利用高精度性能计数器来进行定时，具体方法如下： 下面两个函数是VC提供的仅供Windows 95及其后续版本使用的精确时间函数，并要求计算机从硬件上支持精确定时器。QueryPerformanceFrequency()函数和QueryPerformanceCounter()函数的原型如下： BOOL QueryPerformanceFrequency(LARGE_INTEGER ＊lpFrequency);BOOL QueryPerformanceCounter(LARGE_INTEGER ＊lpCount);数据类型ARGE_INTEGER既可以是一个8字节长的整型数，也可以是两个4字节长的整型数的联合结构， 其具体用法根据编译器是否支持64位而定。该类型的定义如下： typedef union _LARGE_INTEGER{ struct { DWORD LowPart ;// 4字节整型数 LONG HighPart;// 4字节整型数 }; LONGLONG QuadPart ;// 8字节整型数}LARGE_INTEGER ;在进行定时之前，先调用QueryPerformanceFrequency()函数获得机器内部定时器的时钟频率， 然后在需要严格定时的事件发生之前和发生之后分别调用QueryPerformanceCounter()函数，利用两次获得的计数之差及时钟频率，计算出事件经历的精确时间。 #include &lt;windows.h&gt; double PCFreq = 0.0;__int64 CounterStart = 0; void StartCounter(){ LARGE_INTEGER li; if(!QueryPerformanceFrequency(&amp;li)) cout &lt;&lt; “QueryPerformanceFrequency failed!n”; PCFreq = double(li.QuadPart)/1000.0; QueryPerformanceCounter(&amp;amp;li); CounterStart = li.QuadPart;}double GetCounter(){ LARGE_INTEGER li; QueryPerformanceCounter(&amp;amp;li); return double(li.QuadPart-CounterStart)/PCFreq;} int main(){ StartCounter(); Sleep(1000); cout &lt;&lt; GetCounter() &lt;&lt;”n”; return 0;}使用秒为单位，则使用：PCFreq = double(li.QuadPart); 使用微秒做单位，则使用：PCFreq = double(li.QuadPart)/1000000.0。 同时摘录一些对该API的评论： windows API计时最准的是QueryPerformanceCounter，CPU计时的其他任何时间函数都是用操作系统时间片计时，多核下Windows的时间片是15毫秒，单核10毫秒，决定了你的最高精度10-15毫秒。CPU计时必须有CPU的硬件支持，ARM版WinCE也有QueryPerformanceCounter，但他的精度和时间片计时一样。 新的x86基本都能高精度计时。sunos是最早把高精度计时（微秒级）带入通用操作系统的，因为他的硬件封闭，sparc的CPU有高精度计时的支持。 Windows的GetSystemTime()、GetTickCount()、GetSystemTimeAsFileTime()等函数的值是存放在系统的固定内存区域的，每次时钟中断时由操作系统更新一次（更新周期即系统任务调度时间片，一般为5到15毫秒）。Windows这么处理是基于效率考虑的，每次做真正的时间计算需要耗费一定时间（读硬件时钟、计算年月日等）。Linux的gettimeofday()比较精确，每次都做准确计算，但比较低效（一次调用要耗费1万多cpu周期）]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 代理配置]]></title>
    <url>%2F2013%2F09%2F05%2Fgit-proxy%2F</url>
    <content type="text"><![CDATA[最近Git在国内使用时偶尔会遇到被墙的情况，今天我就遇到了，导致无法更新自己的远端Git仓库，于是使用goagent来解决，浏览器端的代理配置很简单，相关goagent的wiki页面也有很详细的解释。 Linux/Mac系统下的goagent的使用教程 现在你应该可以在Terminal下利用Python运行goagent代理了，代理地址是 127.0.0.1:8087，下面就以使用goagent为例，介绍如何为git配置代理。 Git 目前支持的三种协议 git://、ssh:// 和 http://，其代理配置各不相同：core.gitproxy 用于git:// 协议，http.proxy 用于 http:// 协议，ssh:// 协议的代理需要配置 ssh 的 ProxyCommand 参数。 先从最简单的HTTP和HTTPS协议配置代理开始，在Linux下，用户可以通过编辑自己的.bash_profile或.bash_rc文件来配置全局代理，或者直接修改环境变量，使用以下命令即可： export http_proxy=https://127.0.0.1:8087 export https_proxy=https://127.0.0.1:8087 ftp_proxy变量也是可以设置的，不过对git没用。记得在更新完.bash_profile或.bash_rc文件后运行一下： source .bash_profile 另外，你也可以利用git config来单独为git配置代理，方法如下： git config –global https.proxy http://127.0.0.1:8087git config –global http.proxy http://127.0.0.1:8087 &nbsp; ~/.gitconfig 文件 ：具体到你的用户。你可以通过传递–global 选项使Git 读或写这个特定的文件。这里运行之后可以在用户目录的.gitconfig文件中看到修改效果，在那里也可以去除这些配置。 以上方法可以解决使用HTTPS协议的代理问题，对于我自己的项目，一般使用的都是HTTPS协议，在git clone项目的时候我们可以注意后面的url，看使用的是什么类型的协议。 如果是git协议，可以使用git config修改core.gitproxy，这里需要为其指定一个脚本文件，来执行代理的操作，例如我们建立git-proxy.sh文件，文件内容为： #!/bin/shconnect -S 127.0.0.1:7070 “$@”如果没有connect命令的话，可以apt-get安装 proxy-connect 或者 connect-proxy 软件包即可。然后执行：export GIT_PROXY_COMMAND=”/path/to/git-proxy.sh“或者git config –global core.gitproxy “/path/to/git-proxy.sh”最后是针对SSH协议的配置，建立git-proxy-ssh.sh文件，写入： #!/bin/shssh -o ProxyCommand=”/path/to/git-proxy.sh %h %p” “$@”然后配置git使用该文件：export GIT_SSH=”/path/to/git-proxy-ssh.sh“当然也可以直接配置 ～/.ssh/config 的 ProxyCommand，例如Host github.comProxyCommand nc -X 5 -x 127.0.0.1:8087 %h %p这样链接到 github.com 的 ssh 都会使用 127.0.0.1:8087 的代理，ssh 的配置可以参考 man ssh_config ，nc 命令的使用可以参考 man nc 。 SSH 协议的代理可以是全局的（去掉 Host 那行就行了），也可以针对某个网站，但不能针对某个仓库配置。]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端开发工具 Bootstrap 3.0 正式版发布]]></title>
    <url>%2F2013%2F08%2F23%2Fbootstrap-3-0-release%2F</url>
    <content type="text"><![CDATA[不久前我接触了Bootstrap 2.3.2，对于一个像我这样之前没有太多前端开发经验和网页设计经验的人来说，Bootstrap是一个很好的工具，同时基于Bootstrap还有其他扩展和类似的工具包，我也自己做了几个简单的网页（链接），页面元素加入了Google的一些风格。前不久，广受期待的3.0终于与广大开发者见面了。 Bootstrap是Twitter推出的一个开源的用于前端开发的工具包，包含了丰富的Web组件。根据这些组件，开发者可以快速的搭建一个漂亮、功能完备的网站。在经过Bootstrap 3 RC版的测试和改善后，Bootstrap 3.0.0于8月20日正式发布。 Bootstrap 3.0 值得关注的特性包括： 全新设计的风格和可选主题，趋向扁平化设计和提供更多可选主题； 面向移动优先和响应式设计，更好地支持移动端设备的开发； 全新定制，重新设计，用浏览器代替Heroku对其进行编译，更好的依赖支持、内置错误处理等； 默认更好的盒子模型； 超强的网格系统，更好地支持手机、平板电脑、台式机和大屏幕布局； 重写了JavaScript插件； 新的Glyphicons图标字体； 导航条组件的大改进； 模态对话框更好的响应式效果； 组件的维护（新增和删除）； 文档的完善； 不再支持 IE7 和 Firefox 3.6。详细的介绍请看发行说明。 下载地址：http://blog.getbootstrap.com/2013/08/19/bootstrap-3-released/ https://github.com/twbs/bootstrap/releases/download/v3.0.0/bootstrap-3.0.0-dist.zip Github地址：https://github.com/twbs/bootstrap 在线演示：http://twbs.github.io/bootstrap/components/]]></content>
      <categories>
        <category>网站</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>develop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux内存管理详细解析]]></title>
    <url>%2F2013%2F08%2F21%2Flinux-memory%2F</url>
    <content type="text"><![CDATA[原文出处： 赛迪 我是一名程序员，那么我在这里以一个程序员的角度来讲解Linux内存的使用。一提到内存管理，我们头脑中闪出的两个概念，就是虚拟内存，与物理内存。这两个概念主要来自于linux内核的支持。 Linux在内存管理上份为两级，一级是线性区，类似于00c73000-00c88000，对应于虚拟内存，它实际上不占用实际物理内存；一级是具体的物理页面，它对应我们机器上的物理内存。 这里要提到一个很重要的概念，内存的延迟分配。Linux内核在用户申请内存的时候，只是给它分配了一个线性区（也就是虚存），并没有分配实际物理 内存；只有当用户使用这块内存的时候，内核才会分配具体的物理页面给用户，这时候才占用宝贵的物理内存。内核释放物理页面是通过释放线性区，找到其所对应 的物理页面，将其全部释放的过程。 char *p=malloc(2048) //这里只是分配了虚拟内存2048，并不占用实际内存。strcpy(p,”123”) //分配了物理页面，虽然只是使用了3个字节，但内存还是为它分配了2048字节的物理内存。free(p) //通过虚拟地址，找到其所对应的物理页面，释放物理页面，释放线性区。我们知道用户的进程和内核是运行在不同的级别，进程与内核之间的通讯是通过系统调用来完成的。进程在申请和释放内存，主要通过brk,sbrk,mmap,unmmap这几个系统调用，传递的参数主要是对应的虚拟内存。 注意一点，在进程只能访问虚拟内存，它实际上是看不到内核物理内存的使用，这对于进程是完全透明的。 glibc内存管理器那么我们每次调用malloc来分配一块内存，都进行相应的系统调用呢？ 答案是否定的，这里我要引入一个新的概念，glibc的内存管理器。 我们知道malloc和free等函数都是包含在glibc库里面的库函数，我们试想一下，每做一次内存操作，都要调用系统调用的话，那么程序将多么的低效。 实际上glibc采用了一种批发和零售的方式来管理内存。glibc每次通过系统调用的方式申请一大块内存（虚拟内存），当进程申请内存时，glibc就从自己获得的内存中取出一块给进程。 内存管理器面临的困难我们在写程序的时候，每次申请的内存块大小不规律，而且存在频繁的申请和释放，这样不可避免的就会产生内存碎块。而内存碎块，直接会导致大块内存申 请无法满足，从而更多的占用系统资源；如果进行碎块整理的话，又会增加cpu的负荷，很多都是互相矛盾的指标，这里我就不细说了。 我们在写程序时，涉及内存时，有两个概念heap和stack。传统的说法stack的内存地址是向下增长的，heap的内存地址是向上增长的。 函数malloc和free，主要是针对heap进行操作，由程序员自主控制内存的访问。 在这里heap的内存地址向上增长，这句话不完全正确。 glibc对于heap内存申请大于128k的内存申请，glibc采用mmap的方式向内核申请内存，这不能保证内存地址向上增长；小于128k的则采用brk，对于它来讲是正确的。128k的阀值，可以通过glibc的库函数进行设置。 这里我先讲大块内存的申请，也即对应于mmap系统调用。 对于大块内存申请，glibc直接使用mmap系统调用为其划分出另一块虚拟地址，供进程单独使用；在该块内存释放时，使用unmmap系统调用将这块内存释放，这个过程中间不会产生内存碎块等问题。 针对小块内存的申请，在程序启动之后，进程会获得一个heap底端的地址，进程每次进行内存申请时，glibc会将堆顶向上增长来扩展内存空间，也 就是我们所说的堆地址向上增长。在对这些小块内存进行操作时，便会产生内存碎块的问题。实际上brk和sbrk系统调用，就是调整heap顶地址指针。 &nbsp; 那么heap堆的内存是什么时候释放呢？当glibc发现堆顶有连续的128k的空间是空闲的时候，它就会通过brk或sbrk系统调用，来调整heap顶的位置，将占用的内存返回给系统。这时，内核会通过删除相应的线性区，来释放占用的物理内存。 下面我要讲一个内存空洞的问题： 一个场景，堆顶有一块正在使用的内存，而下面有很大的连续内存已经被释放掉了，那么这块内存是否能够被释放？其对应的物理内存是否能够被释放？ 很遗憾，不能。 这也就是说，只要堆顶的部分申请内存还在占用，我在下面释放的内存再多，都不会被返回到系统中，仍然占用着物理内存。为什么会这样呢？ 这主要是与内核在处理堆的时候，过于简单，它只能通过调整堆顶指针的方式来调整调整程序占用的线性区；而又只能通过调整线性区的方式，来释放内存。所以只要堆顶不减小，占用的内存就不会释放。 提一个问题： char *p=malloc(2);free(p) 为什么申请内存的时候，需要两个参数，一个是内存大小，一个是返回的指针；而释放内存的时候，却只要内存的指针呢？ 这主要是和glibc的内存管理机制有关。glibc中，为每一块内存维护了一个chunk的结构。glibc在分配内存时，glibc先填写chunk结构中内存块的大小，然后是分配给进程的内存。 chunk ——sizep———— content在进程释放内存时，只要 指针-4 便可以找到该块内存的大小，从而释放掉。 注：glibc在做内存申请时，最少分配16个字节，以便能够维护chunk结构。 glibc提供的调试工具： 为了方便调试，glibc 为用户提供了 malloc 等等函数的钩子（hook），如 __malloc_hook 对应的是一个函数指针， void function (size_t size, const void caller)其中 caller 是调用 malloc 返回值的接受者（一个指针的地址）。另外有 __malloc_initialize_hook函数指针，仅仅会调用一次（第一次分配动态内存时）。（malloc.h） 一些使用 malloc 的统计量（SVID 扩展）可以用 struct mallinfo 储存，可调用获得。 struct mallinfo mallinfo (void)如何检测 memory leakage？glibc 提供了一个函数 void mtrace (void)及其反作用void muntrace (void) 这时会依赖于一个环境变量 MALLOC_TRACE 所指的文件，把一些信息记录在该文件中 用于侦测 memory leakage，其本质是安装了前面提到的 hook。一般将这些函数用 #ifdef DEBUGGING 包裹以便在非调试态下减少开销。产生的文件据说不建议自己去读， 而使用 mtrace 程序（perl 脚本来进行分析）。下面用一个简单的例子说明这个过程，这是 源程序： #include #include #includeintmain( int argc, char argv[] ){ int p, *q ; #ifdef DEBUGGING mtrace( ) ; #endif p = malloc( sizeof( int ) ) ; q = malloc( sizeof( int ) ) ; printf( “p = %pnq = %pn”, p, q ) ; p = 1 ; q = 2 ; free( p ) ; return 0 ;}很简单的程序，其中 q 没有被释放。我们设置了环境变量后并且 touch 出该文件 执行结果如下： p = 0x98c0378q = ``0x98c0388 该文件内容如下 = Start@./test30:[``0x8048446``] + ``0x98c0378 0x4@./test30:[``0x8048455``] + ``0x98c0388 0x4@./test30:[``0x804848f``] - ``0x98c0378 到这里我基本上讲完了，我们写程序时，数据部分内存使用的问题。 代码占用的内存数据部分占用内存，那么我们写的程序是不是也占用内存呢？ 在linux中，程序的加载，涉及到两个工具，linker 和loader。Linker主要涉及动态链接库的使用，loader主要涉及软件的加载。 exec执行一个程序 elf为现在非常流行的可执行文件的格式，它为程序运行划分了两个段，一个段是可以执行的代码段，它是只读，可执行；另一个段是数据段，它是可读写，不能执行。 loader会启动，通过mmap系统调用，将代码端和数据段映射到内存中，其实也就是为其分配了虚拟内存，注意这时候，还不占用物理内存；只有程序执行到了相应的地方，内核才会为其分配物理内存。 loader会去查找该程序依赖的链接库，首先看该链接库是否被映射进内存中，如果没有使用mmap，将代码段与数据段映射到内存中，否则只是将其加入进程的地址空间。这样比如glibc等库的内存地址空间是完全一样。因此一个2M的程序，执行时，并不意味着为其分配了2M的物理内存，这与其运行了的代码量，与其所依赖的动态链接库有关。 运行过程中链接动态链接库与编译过程中链接动态库的区别我们调用动态链接库有两种方法：一种是编译的时候，指明所依赖的动态链接库，这样loader可以在程序启动的时候，来所有的动态链接映射到内存中；一种是在运行过程中，通过dlopen和dlfree的方式加载动态链接库，动态将动态链接库加载到内存中。 这两种方式，从编程角度来讲，第一种是最方便的，效率上影响也不大，在内存使用上有些差别。 第一种方式，一个库的代码，只要运行过一次，便会占用物理内存，之后即使再也不使用，也会占用物理内存，直到进程的终止。 第二中方式，库代码占用的内存，可以通过dlfree的方式，释放掉，返回给物理内存。 这个差别主要对于那些寿命很长，但又会偶尔调用各种库的进程有关。如果是这类进程，建议采用第二种方式调用动态链接库。 占用内存的测量测量一个进程占用了多少内存，linux为我们提供了一个很方便的方法，/proc目录为我们提供了所有的信息，实际上top等工具也通过这里来获取相应的信息。 /proc/meminfo 机器的内存使用信息/proc/pid/maps pid为进程号，显示当前进程所占用的虚拟地址。/proc/pid/statm 进程所占用的内存[root@localhost ~]# cat /proc/self/statm654 57 44 0 0 334 0输出解释 CPU 以及CPU0。。。的每行的每个参数意思（以第一行为例）为： 参数 解释 /proc//status Size (pages) 任务虚拟地址空间的大小 VmSize/4Resident(pages) 应用程序正在使用的物理内存的大小 VmRSS/4Shared(pages) 共享页数 0Trs(pages) 程序所拥有的可执行虚拟内存的大小 VmExe/4Lrs(pages) 被映像到任务的虚拟内存空间的库的大小 VmLib/4Drs(pages) 程序数据段和用户态的栈的大小 （VmData+ VmStk ）4dt(pages) 04查看机器可用内存 /proc/28248/&gt;freetotal used free shared buffers cachedMem: 1023788 926400 97388 0 134668 503688-/+ buffers/cache: 288044 735744Swap: 1959920 89608 1870312我们通过free命令查看机器空闲内存时，会发现free的值很小。这主要是因为，在linux中有这么一种思想，内存不用白不用，因此它尽可能的cache和buffer一些数据，以方便下次使用。但实际上这些内存也是可以立刻拿来使用的。 所以 空闲内存=free+buffers+cached=total-used 查看进程使用的内存查看一个进程使用的内存，是一个很令人困惑的事情。因为我们写的程序，必然要用到动态链接库，将其加入到自己的地址空间中，但是/proc/pid/statm统计出来的数据，会将这些动态链接库所占用的内存也简单的算进来。 这样带来的问题，动态链接库占用的内存有些是其他程序使用时占用的，却算在了你这里。你的程序中包含了子进程，那么有些动态链接库重用的内存会被重复计算。 因此要想准确的评估一个程序所占用的内存是十分困难的，通过写一个module的方式，来准确计算某一段虚拟地址所占用的内存，可能对我们有用。(T002)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小论文 选题]]></title>
    <url>%2F2013%2F08%2F21%2Fessay-title%2F</url>
    <content type="text"><![CDATA[在毕业之前是必须有一篇小论文必须发表的，通常情况是可以从毕业设计大论文的选题里出，但是对于太阳能控制这种工程性的问题，实在想不出什么大的研究价值，所以还是决定另选题目了。其实还是希望导师可以从一些研究方向予以指导，至少指导一下大的方向和题目的，自己做起来还是有点迷茫，而且有选择困难症。 下面把几个可能的方向列举一下： 推荐系统 基于GPU的推荐算法（协同过滤）的加速 随机数生成算法，和基于GPU的加速 NOSQL Database]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>thesis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何评价随机数生成算法]]></title>
    <url>%2F2013%2F08%2F21%2Frandom-number%2F</url>
    <content type="text"><![CDATA[部分内容转自知乎 http://www.zhihu.com/question/20222653，在此做一个记录和总结。 主要的评价标准可以参考德国联邦信息安全办公室给出了随机数发生器质量评判的四个标准。 四个判别随机数序列质量的准则： K1 — A sequence of random numbers with a low probability of containing identical consecutive elements. K2 — A sequence of numbers which is indistinguishable from ‘true random’ numbers according to specified statistical tests. K3 — It should be impossible for any attacker (for all practical purposes) to calculate, or otherwise guess, from any given sub-sequence, any previous or future values in the sequence, nor any inner state of the generator. K4 — It should be impossible, for all practical purposes, for an attacker to calculate, or guess from an inner state of the generator, any previous numbers in the sequence or any previous inner generator states.翻译如下： K1——出现相同连续元素的随机数序列的概率较低。 K2——按照特定的统计学测试，无法区分生成的随机数与真随机数。符合统计学的平均性，比如所有数字出现概率应该相同，卡方检验应该能通过，超长游程长度概略应该非常小，自相关应该只有一个尖峰，任何长度的同一数字之后别的数字出现概率应该仍然是相等的等等。 K3——从一段已知随机数序列计算或者猜测出随机数发生器的内部工作状态或者上一个或下一个随机数，应该是不可能的。 K4——从随机数发生器的工作状态猜测出随机数发生器以前的工作状态，或序列中前面的随机数，应该是不可能的。我们一般用的随机数发生器至少要符合K1和K2，而用于加密等应用的随机数发生器则还要符合K3和K4。 有一系列的测试可以判断一个随机数生成器的优劣。NIST发布了一个测试工具包专门来做这件事情。http://csrc.nist.gov/groups/ST/toolkit/rng/documentation_software.html 还有一些其他应用随机数计算与理论值对比的方法，例如用蒙特卡洛方法求高维数值积分（例如n维球的体积和面积），如果随机数生成器足够好，积分结果应该能很好地逼近理论值。用蒙特卡洛方法计算PI也可以是一种尝试。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>random number</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解Git的工作流程 【转载】]]></title>
    <url>%2F2013%2F08%2F20%2Fgit-workflow%2F</url>
    <content type="text"><![CDATA[英文原文：Understanding the Git workflow 编译： 张重骐 如果你不理解Git的设计动机，那你就会处处碰壁。知道足够多的命令和参数后，你就会强行让Git按你想的来工作，而不是按Git自己的方式来。这就像把螺丝刀当锤子用；也能把活干完，但肯定干的差极了，花费很长时间，还会弄坏螺丝刀。 想想常见的Git工作流程是怎么失效的吧。 从Master创建一个分支，写代码，然后把这个分支合并回Master。多数时候这样做的效果会如你所愿，因为从你创建分支到合并回去之间，Master一般都会有些变动。然后，有一天当你想把一个功能 （feature）分支合并进Master的时候，而Master并没有像以往那样有变动，问题来了：这时Git不会进行合并commit，而是将 Master指向功能分支上的最新commit。（看图） 不幸的是，你的功能分支有用来备份代码的commit（作者称之为checkpoint commit），这些经常进行的commit对应的代码可能处于不稳定状态！而这些commit现在没法和Master上那些稳定的commit区分开来 了。当你想回滚的时候，很容易发生灾难性后果。 于是你就记住了：“当合并功能分支的时候，加上 -no-ff 选项强制进行一次全新的commit。”嗯，这么做好像解决问题了，那么继续。 然后一天你在线上环境中发现了一个严重bug，这时你需要追溯下这个bug是什么时候引入的。你运行了bisect命令，但却总是追溯到一些不稳定的commit。因此你不得不放弃，改用人肉检查。 最后你将bug范围缩小到一个文件。你运行blame命令查看这个文件在过去48小时里的变动。然后blame告诉你这个文件已经好几周没有被修改 过了——你知道根本不可能没有变动。哦，原来是因为blame计算变动是从第一次commit算起，而不是merge的时候。你在几周前的一次 commit中改动了这个文件，但这个变动今天才被merge回来。 用no-ff来救急，bisect又临时失效，blame的运作机制又那么模糊，所有这些现象都说明一件事儿，那就是你正在把螺丝刀当锤子用。 反思版本控制 版本控制的存在是因为两个原因。 首先，版本控制是用来辅助写代码的。因为你要和同事同步代码，并经常备份自己的代码。当然了，把文件压缩后发邮件也行，不过工程大了大概就不好办了。 其次，就是辅助配置管理工作。其中就包括并行开发的管理，比如一边给线上版本修复bug，一边开发下一个版本。配置管理也可以帮助弄清楚变动发生的具体时间，在追溯bug中是一个很好的工具。 一般说来，这两个原因是冲突的。 在开发一个功能的时候，你应该经常做备份性的commit。然而，这些commit经常会让软件没法编译。 理想情况是，你的版本更新历史中的每一次变化都是明确且稳定的，不会有备份性commit带来的噪声，也不会有超过一万行代码变动的超大 commit。一个清晰的版本历史让回滚和选择性merge都变得相当容易，而且也方便以后的检查和分析。然而，要维护这样一个干净的历史版本库，也许意 味着总是要等到代码完善之后才能提交变动。 那么，经常性的commit和干净的历史，你选择哪一个？ 如果你是在刚起步的创业公司中，干净的历史没有太大帮助。你可以放心地把所有东西都往Master中提交，感觉不错的时候随时发布。 如果团队规模变大或是用户规模扩大了，你就需要些工具和技巧来做约束，包括自动化测试，代码检查，以及干净的版本历史。 功能分支貌似是一个不错的折中选择，能够基本的并行开发问题。当你写代码时候，可以不用怎么在意集成的问题，但它总有烦到你的时候。 当你的项目规模足够大的时候，简单的branch/commit/merge工作流程就出问题了。缝缝补补已经不行了。这时你需要一个干净的版本历史库。 Git之所以是革命性的，就是因为它能同时给你这两方面的好处。你可以在原型开发过程中经常备份变动，而搞定后只需要交付一个干净的版本历史。 &nbsp; 工作流程 考虑两种分支：公共的和私有的。 公共分支是项目的权威性历史库。在公共分支中，每一个commit都应该确保简洁、原子性，并且有完善的提交信息。此分支应该尽可能线性，且不能更改。公共分支包括Master和发行版的分支。 私有分支是供你自己使用的，就像解决问题时的草稿纸。 安全起见，把私有分支只保存在本地。如果你确实需要push到服务器的话（比如要同步你在家和办公室的电脑），最好告诉同事这是私有的，不要基于这个分支展开工作。 绝不要直接用merge命令把私有分支合并到公共分支中。要先用reset、rebase、squash merges、commit amending等工具把你的分支清理一下。 把你自己看做一个作者，每一次的commit视为书中的一章。作者不会出版最初的草稿，就像Michael Crichton说的，“伟大的书都不是写出来——而是改出来的”。 如果你没接触过Git，那么修改历史对你来说好像是种禁忌。你习惯于认为提交过的所有东西都应该像刻在石头上一样不能抹去。但如果按这种逻辑，我们在文本处理软件器中也不应该使用“撤销”功能了。 实用主义者们直到变化变为噪音的时候才关注变化。对于配置管理来说，我们关注宏观的变化。日常commit（checkpoint commits）只是备份于云端的用于“撤销”的缓冲。 如果你保持公共历史版本库的简洁，那么所谓的fast-forward merge就不仅安全而且可取了，它能保证版本变更历史的线性和易于追溯。 关于 -no-ff 仅剩的争论就只剩“文档证明”了。人们可能会先merge再commit，以此代表最新的线上部署版本。不过，这是反模式的。用tag吧。 规则和例子 根据改变的多少、持续工作时间的长短，以及分支分叉了多远，我使用三种基本的方法。 1）短期工作 绝大多数时间，我做清理时只用squash merge命令。 假设我创建了一个功能分支，并且在接下来一个小时里进行了一系列的checkpoint commit。 git checkout -b private_feature_branchtouch file1.txtgit add file1.txtgit commit -am “WIP”完成开发后，我不是直接执行git merge命令，而是这样： git checkout mastergit merge –squash private_feature_branchgit commit -v然后我会花一分钟时间写个详细的commit日志。 2）较大的工作 有时候一个功能可以延续好几天，伴有大量的小的commit。 我认为这些改变应该被分解为一些更小粒度的变更，所以squash作为工具来说就有点儿太糙了。（根据经验我一般会问，“这样能让阅读代码更容易吗？”） 如果我的checkpoint commits之后有合理的更新，我可以使用rebase的交互模式。 交互模式很强大。你可以用它来编辑、分解、重新排序、合并以前的commit。 在我的功能分支上： git rebase –interactive master 然后会打开一个编辑器，里边是commit列表。每一行上依次是，要执行的操作、commit的SHA1值、当前commit的注释。并且提供了包含所有可用命令列表的图例。 默认情况下，每个commit的操作都是“pick”，即不会修改commit。 pick ccd6e62 Work on back button pick 1c83feb Bug fixes pick f9d0c33 Start work on toolbar我把第二行修改为“squash”，这样第二个commit就会合并到第一个上去。 pick ccd6e62 Work on back button squash 1c83feb Bug fixes pick f9d0c33 Start work on toolbar 保存并退出，会弹出一个新的编辑器窗口，让我为本次合并commit做注释。就这样。 舍弃分支 也许我的功能分支已经存在了很久很久，我不得不将好几个分支合并进这个功能分支中，以便当我写代码时这个分支是足够新的的。版本历史让人费解。最简单的办法是创建一个新的分支。 git checkout mastergit checkout -b cleaned_up_branchgit merge –squash private_feature_branchgit reset现在，我就有了一个包含我所有修改且不含之前分支历史的工作目录。这样我就可以手动添加和commit我的变更了。 总结 如果你在与Git的默认设置背道而驰，先问问为什么。 将公共分支历史看做不可变的、原子性的、容易追溯的。将私有分支历史看做一次性的、可编辑的。 推荐的工作流程是： 基于公共分支创建一个私有分支。 经常向这个私有分支commit代码。 一旦你的代码完善了，就清理掉下私有分支的历史。 将干净的私有分支merge到公共分支中。英文原文：Understanding the Git workflow 编译： 张重骐]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>develop</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordpress 子主题]]></title>
    <url>%2F2013%2F08%2F19%2Fwordpress-child-theme%2F</url>
    <content type="text"><![CDATA[本站采用的是wordpress官方发布的TwentyEleven主题，官方会发布主题更新，但是如果你对主题文件进行了修改的话，每次更新后，原来的修改就丢失了，解决问题的最好方法就是采用子主题（Child Theme）。 Child Theme创建一个子主题是很简单的。 在wp-content/themes/建立新的文件夹，例如建立 twentyeleven-childen。 创建一个目录，将格式编写正确的 style.css 文件放进去，一个子主题就做成了！只需要对 HTML 和CSS 具有基本的了解，您就可以通过创建一个非常基本的子主题 来对一个父主题的样式和布局进行修改和扩展，而不需要对父主题的文件作任何修改。通过这样的方式，当父主题被更新的时候，您所做的修改就可以保存下来。 因为这个原因，我们强烈推荐您使用子主题的方式来对主题进行修改。 必需的style.css文件style.css是一个子主题唯一必须的文件。它的头部提供的信息让WordPress辨认出子主题，并且重写父主题中的style.css文件。 对于任何WordPress主题，头部信息必须位于文件的顶端，唯一的区别就是子主题中的Template:行是必须的，因为它让WordPress知道子主题的父主题是什么。 下面是一个style.css文件的头部信息的示例： /* Theme Name: Twenty Ten Child Theme URI: http: //example.com/ Description: Child theme for the Twenty Ten theme Author: Your name here Author URI: http: //example.com/about/ Template: twentyten Version: 0.1.0 */ 逐行的简单解释： * `Theme Name`. (**必需**) 子主题的**名称**。 * `Theme URI`. (可选) 子主题的主页。 * `Description`. (可选) 子主题的描述。比如：我的第一个子主题，真棒！ * `Author URI`. (可选) 作者主页。 * `Author`. (optional) 作者的名字。 * `Template`. (**必需**) 父主题的目录名，区别大小写。 **注意：** 当你更改子主题名字时，要先换成别的主题。 * `Version`. (可选) 子主题的版本。比如：0.1，1.0，等。 `*/ `这个关闭标记的后面部分，就会按照一个常规的样式表文件一样生效，你可以把你想对WordPress应用的样式规则都写在它的后面。 要注意的是，子主题的样式表会替换父主题的样式表而生效。（事实上WordPress根本就不会载入父主题的样式表。）所以，如果你想简单地改变父主题中的一些样式和结构——而不是从头开始制作新主题——你必须明确的导入父主题的样式表，然后对它进行修改。下面的例子告诉你如何使用`@import`规则完成这个。例如 @import url("../twentyten/style.css"); 使用 functions.php不像style.css，子主题中的functions.php不会覆盖父主题中对应功能，而是将新的功能加入到父主题的functions.php中。（其实它会在父主题文件加载之前先载入。） 完整的Child Theme說明可以參考英文說明或簡體中文說明。]]></content>
      <categories>
        <category>网站</category>
      </categories>
      <tags>
        <tag>web</tag>
        <tag>wordpress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈二代身份证缺陷]]></title>
    <url>%2F2013%2F08%2F19%2Fid-card-problem%2F</url>
    <content type="text"><![CDATA[近期二代身份证“先天缺陷”的问题闹得沸沸扬扬，尤其是公安部已经做出了正式回应，要求加快推进登记指纹信息工作。那么从技术角度上来说，所谓的“先天缺陷”又是如何来的呢？ 第二代身份证的工作原理第二代身份证的夹层中有芯片和线圈，当身份证阅读机发射电磁波读卡信号后，二代身份证的线圈感应了电磁波产生电流，芯片在电流的作用下开始工作，将二代身份证信息以电磁波发回给阅读机，这样阅读机就知道了二代身份证上的信息 。在我国，居民生活中丢失身份证后，即使做补办（挂失）处理，但由于还没有任何注销措施，导致原身份证仍可正常使用。大量遗失、被盗身份证正通过网络进行非 法交易，并被广泛用于开办银行卡、信用卡，掩护诈骗、洗钱活动；更令人忧虑的是，由于缺乏必要的密码等基本防伪功能，若不法分子掌握与自己外貌相近的他人 真实身份证，则可“分身两人”，加大公安机关打击犯罪的难度。 二代身份证的防伪性能不错，也就是说很难仿制出一张一样可以用的身份证，但是可以说完全没有防盗的措施，任何人捡到你的身份证都可以拿去用，相信也有不少人有多张身份证（例如户口迁移加办的）可以同时使用的经历，可见当时设计时实在考虑欠周到。 公安部正在大力推进的指纹加密是个很好的方法，但同时就加上了设备的需求，机读设备必须同时配备指纹读取设备，且不说成本如何，以前的设备就是个简单的IC卡读取，一下就变复杂了不少吧。这里你要仔细想想公安部其实有更深入的目的，例如，在一般情况下，只会对犯罪分子采取登记指纹的措施，这下来个全民登记，录入数据库，以后破案会有不少帮助吧，当然也可以达到其他的一些目的。 从技术上讲，我也想过两个方法：1，身份证信息简单加个有效时间戳，那么丢失的无效的时间戳就会比较旧，读取的时候与中心数据库做个对比就行了，但仔细一想，各个终端机读设备应该不会有直接联网到公安部数据库读取的权力，而且这样系统复杂不少，一般情况下都是直接读取身份证信息的，中心管理模型有不少好处，例如监控非法读取设备、身份证的使用情况等，有点像银行的ATM，身份证在哪一用、一登记，立马就能监控到，但是与原来系统相比改动太大；2，密码加密，设备相当简单，还是比较实用的，与指纹加密的方法相同。其实在设计之初就应该采用密码加密，逐步过渡到生物技术加密。 生物信息加密的好处不言而喻，按我的设想，未来的身份证应该可以直接使用人的生物信息，有点像科幻电影《少数派报告》的场景，依靠扫描人眼睛的虹膜来鉴别身份，根据虹膜信息去中央数据库比对，到处都有可以扫描虹膜的监视器去判别身份，行为无处可藏，可比登记指纹要直接的多了！]]></content>
      <categories>
        <category>日志</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[响应式设计的免费测试工具]]></title>
    <url>%2F2013%2F08%2F18%2Fresponsive-tools-test%2F</url>
    <content type="text"><![CDATA[原文链接： Steve Ralston 翻译： 伯乐在线 - 伯乐在线读者译文链接： http://blog.jobbole.com/45535/ deviceponsivedeviceponsive与Am I Responsive?非常相似，它们都简单明了的展示了你的网站，而且对设备而言，都没有可见的控制和选项。所有设备在一页长网页上同时显示。你能够通 过改变背景颜色和嵌入你自己的logo来定制这个网站的页眉，之后截屏分享出去，这十分有趣。从某种方式来说，当你向顾客分享你的截屏的时候，也就帮这个 网站打了广告。 该网站所提供设备及其屏幕大小： _Macbook——1280×800 _iPad portrait——768×1024 _iPad landscape——1024×768 _Kindle portrait——600×1024 _Kindle landscape——1024×600 _iPhone portrait——320×480 _iPhone landscape——480×320 _Galasy portrait——240×320 * Galaxy landscape——320×240 使用这些工具时，大部分情况下，滑动条会在较小的设备上显示。然而在实际的设备上，滑动条不会显示。不过为了测试试图能在不支持触控的设备上也能滑动，必须要做出一些让步。 ScreenflyScreenfly 实 实在在提升了可用系数。它提供九种比平板更大的设备，从10寸的笔记本到24寸的台式电脑都有，此外还包括五种平板，九种手机，三种电视，还能够自己定制 大小。通过另一个控制器，任何选取的设备都能被旋转成水平或者竖直的。你能够选择是否允许滚动，你还能生成一个可用于分享的链接，只需要点一个按钮就行。 这个网站显示分辨率的方式非常十分有益。每一个在菜单中的设备都显示了名称和分辨率，浏览器的实际分辨率在接近右上角的地方，被选中的设备的分辨率则在展示区域的页脚，跟测试网站的URL写在一起。这一个小细节在文档截图和给客户分享信息时给人非常好的感觉。 之前提到的这些足以使它成为一个完美的工具，但Screenfly的开发者还为它升级了代理服务器的特性，并认为非常合适。用写网站上的话来 说，”Screenfly能使用一个代理服务器，在你访问你的网站时伪装成其他设备。代理服务器模仿你所选择的设备的用户代理字符串，而不是该设备的行 为。” 对于其他所有工具，处理这个地方时都仅仅是利用CSS。Screenfly是唯一一个允许基于代理字符串来测试的。 我给一个我自己写的，提供了手机版本的网页做了基于代理字符串的测试，手机版网页的结果非常好。所有效果都跟我想的一样，所有功能也都能通过测试。测试代理字符串是保守的，这一点无可否认，不过这个网站就是这样保守的风格，而且代理服务器的特性也的确给网站带来了好处。 原文中列举了6个免费的测试工具，实际使用了这两个工具，可以满足一般的测试需求，那么你喜欢使用哪个工具呢？]]></content>
      <categories>
        <category>网站</category>
      </categories>
      <tags>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World 代码示例]]></title>
    <url>%2F2013%2F08%2F18%2Fhello-world-code%2F</url>
    <content type="text"><![CDATA[在这里测试一下代码显示插件，以下为代码内容： #include &lt;stdio.h&gt; int main(){ printf(“Hello Worldn”); return 0;} ……&nbsp;]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NoSQL 数据库简介]]></title>
    <url>%2F2013%2F07%2F31%2Fintro-to-nosql%2F</url>
    <content type="text"><![CDATA[NoSQL 数据库的概念是相对于传统的关系型数据库（RDBMS-Relational Database Management System）而言，在近年来获得了极大的发展，被许多人认为是下一代的数据库。NoSQL数据库一般关注以下几点： being non-relational, distributed, open-source and horizontally scalable 非关系型，分布式，开源，水平伸缩性。在大数据和实时web应用方面，NoSQL技术已经很好的找到了自己的位置，并显示出了相对于传统关系型数据库的巨大优势。NoSQL在英文也常常被认为是“Not Only SQL”（不仅仅是SQL），用来强调NoSQL并不是SQL技术的对立面，它也允许类SQL查询语言的使用。 历史：NoSQL一词最早是由Carlo Strozzi用来命名他开发的一个轻量级开源的关系型数据库，这个数据没有提供标准的SQL接口，所以就简单叫做NoSQL，他认为，现在的NoSQL运动，主要是不使用关系模型，应该叫做NoREL（非关系型）更为合适。 在2009年初，Eric Evans再次提出这个概念，那时Johan Oskarsson想组织一个会议来讨论开源的分布式数据库。这个名字用来标记那些越来越多的非关系型、分布式数据存储、并且不尝试提供ACID（原子性、一致性、隔离性和持久性）的数据库。 关系型数据库最大特点就是事务的一致性：传统的关系型数据库读写操作都是事务的，具有ACID（原子性Atomicity、一致性Consistency、隔离性Isolation、持久性Durability）的特点，C就是一致性（Consistency），这个特点是关系型数据库的灵魂（其他三个AID都是为其服务的），这个特性使得关系型数据库可以用于几乎所有对一致性有要求的系统中，如典型的银行系统。自2009年初开始，NoSQL运动发展迅猛，现在已经有了大约150种NoSQL数据库，你可以在这个网站http://nosql-database.org/查看其中一些比较有代表性的NoSQL数据库。 分类大多数人比较认同的分类方法是根据其数据模型进行分类： Column: HBase, Accumulo Document: MongoDB, Couchbase Key-value : Dynamo “Dynamo (storage system)”), Riak, Redis, Cache, Project Voldemort Graph: Neo4J, Allegro, Virtuoso初学者建议学习和使用一下MongoDB和Redis。 参考阅读： 更加详细的NoSQL介绍：http://en.wikipedia.org/wiki/NoSQL 要了解和学习更多的NoSQL数据库，请看：http://nosql-database.org/ NoSQL相关博客：http://nosql.mypopescu.com/ NoSQL相关博客：http://blog.nosqlfan.com/ redis设计与实现：http://www.redisbook.com/en/latest/]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux新手非常有用的20个命令]]></title>
    <url>%2F2013%2F07%2F19%2Flinux-commands%2F</url>
    <content type="text"><![CDATA[原文出处： Tecmit 译文出处： oschina 你打算从Windows换到Linux上来，还是你刚好换到Linux上来？哎哟！！！我说什么呢，是什么原因你就出现我的世界里了。从我以往的经 验来说，当我刚使用Linux，命令，终端啊什么的，吓了我一跳。我担心该记住多少命令，来帮助我完成所有任务。毫无疑问，在线文档，书籍，man pages以及社区帮了我一个大忙，但是我还是坚信有那么一篇文章记录了如何简单学习和理解命令的秘籍。这激发了我掌握Linux和使它容易使用的积极 性。本文就是通往那里的阶梯。 1. ls命令ls命令是列出目录内容(List Directory Contents)的意思。运行它就是列出文件夹里的内容，可能是文件也可能是文件夹。 root@tecmint:~# ls Android-Games MusicPictures PublicDesktop Tecmint.comDocuments TecMint-SyncDownloads Templates“ls -l”命令已详情模式(long listing fashion)列出文件夹的内容。 root@tecmint:~# ls -l total 40588drwxrwxr-x 2 ravisaive ravisaive 4096 May 8 01:06 Android Gamesdrwxr-xr-x 2 ravisaive ravisaive 4096 May 15 10:50 Desktopdrwxr-xr-x 2 ravisaive ravisaive 4096 May 16 16:45 Documentsdrwxr-xr-x 6 ravisaive ravisaive 4096 May 16 14:34 Downloadsdrwxr-xr-x 2 ravisaive ravisaive 4096 Apr 30 20:50 Musicdrwxr-xr-x 2 ravisaive ravisaive 4096 May 9 17:54 Picturesdrwxrwxr-x 5 ravisaive ravisaive 4096 May 3 18:44 Tecmint.comdrwxr-xr-x 2 ravisaive ravisaive 4096 Apr 30 20:50 Templates&nbsp; “ls -a”命令会列出文件夹里的所有内容，包括以”.”开头的隐藏文件。 注意：在Linux中，文件以“.”开头的就是隐藏文件，并且每个文件，文件夹，设备或者命令都是以文件对待。ls -l 命令输出： d (代表了是目录). rwxr-xr-x 是文件或者目录对所属用户，同一组用户和其它用户的权限。 上面例子中第一个ravisaive 代表了文件文件属于用户ravisaive 上面例子中的第二个ravisaive代表了文件文件属于用户组ravisaive 4096 代表了文件大小为4096字节. May 8 01:06 代表了文件最后一次修改的日期和时间. 最后面的就是文件/文件夹的名字更多”ls”例子请查看 15 linux中ls命令实例 2. lsblk命令“lsblk”就是列出块设备。除了RAM外，以标准的树状输出格式，整齐地显示块设备。 root@tecmint:~# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 232.9G 0 disk├─sda1 8:1 0 46.6G 0 part /├─sda2 8:2 0 1K 0 part├─sda5 8:5 0 190M 0 part /boot├─sda6 8:6 0 3.7G 0 part [SWAP]├─sda7 8:7 0 93.1G 0 part /data└─sda8 8:8 0 89.2G 0 part /personalsr0 11:0 1 1024M 0 rom“lsblk -l”命令以列表格式显示块设备(而不是树状格式)。 root@tecmint:~# lsblk -l NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 232.9G 0 disksda1 8:1 0 46.6G 0 part /sda2 8:2 0 1K 0 partsda5 8:5 0 190M 0 part /bootsda6 8:6 0 3.7G 0 part [SWAP]sda7 8:7 0 93.1G 0 part /datasda8 8:8 0 89.2G 0 part /personalsr0 11:0 1 1024M 0 rom注意：lsblk是最有用和最简单的方式来了解新插入的USB设备的名字，特别是当你在终端上处理磁盘/块设备时。 3. md5sum命令“md5sum”就是计算和检验MD5信息签名。md5 checksum(通常叫做哈希)使用匹配或者验证文件的文件的完整性，因为文件可能因为传输错误，磁盘错误或者无恶意的干扰等原因而发生改变。 root@tecmint:~# md5sum teamviewer_linux.deb 47790ed345a7b7970fc1f2ac50c97002 teamviewer_linux.deb注意：用户可以使用官方提供的和md5sum生成签名信息匹对以此检测文件是否改变。Md5sum没有sha1sum安全，这点我们稍后讨论。 4. dd命令“dd”命令代表了转换和复制文件。可以用来转换和复制文件，大多数时间是用来复制iso文件(或任何其它文件)到一个usb设备(或任何其它地方)中去，所以可以用来制作USB启动器。 root@tecmint:~# dd if=/home/user/Downloads/debian.iso of=/dev/sdb1 bs=512M; sync注意：在上面的例子中，usb设备就是sdb1（你应该使用lsblk命令验证它，否则你会重写你的磁盘或者系统），请慎重使用磁盘的名，切忌。 dd 命令在执行中会根据文件的大小和类型 以及 usb设备的读写速度，消耗几秒到几分钟不等。 5. uname命令“uname”命令就是Unix Name的简写。显示机器名，操作系统和内核的详细信息。 root@tecmint:~# uname -a Linux tecmint 3.8.0-19-generic #30-Ubuntu SMP Wed May 1 16:36:13 UTC 2013 i686 i686 i686 GNU/Linux注意： uname显示内核类别， uname -a显示详细信息。上面的输出详细说明了uname -a “Linux“: 机器的内核名 “tecmint“: 机器的分支名 “3.8.0-19-generic“: 内核发布版本 “#30-Ubuntu SMP“: 内核版本 “i686“: 处理器架构 “GNU/Linux“: 操作系统名 6. history命令“history”命令就是历史记录。它显示了在终端中所执行过的所有命令的历史。 root@tecmint:~# history 1 sudo add-apt-repository ppa:tualatrix/ppa 2 sudo apt-get update 3 sudo apt-get install ubuntu-tweak 4 sudo add-apt-repository ppa:diesch/testing 5 sudo apt-get update 6 sudo apt-get install indicator-privacy 7 sudo add-apt-repository ppa:atareao/atareao 8 sudo apt-get update 9 sudo apt-get install my-weather-indicator 10 pwd 11 cd &amp;&amp; sudo cp -r unity/6 /usr/share/unity/ 12 cd /usr/share/unity/icons/ 13 cd /usr/share/unity&nbsp; 注意：按住“CTRL + R”就可以搜索已经执行过的命令，它可以你写命令时自动补全。 7. sudo命令“sudo”(super user do)命令允许授权用户执行超级用户或者其它用户的命令。通过在sudoers列表的安全策略来指定。 root@tecmint:~# sudo add-apt-repository ppa:tualatrix/ppa 注意：sudo 允许用户借用超级用户的权限，然而”su”命令实际上是允许用户以超级用户登录。所以sudo比su更安全。并不建议使用sudo或者su来处理日常用途，因为它可能导致严重的错误如果你意外的做错了事，这就是为什么在linux社区流行一句话： “To err is human, but to really foul up everything, you need root password.”“人非圣贤孰能无过，但是拥有root密码就真的万劫不复了。” 8. mkdir命令“mkdir”(Make directory)命令在命名路径下创建新的目录。然而如果目录已经存在了，那么它就会返回一个错误信息”不能创建文件夹，文件夹已经存在了”(“cannot create folder, folder already exists”) root@tecmint:~``# mkdir tecmint 注意：目录只能在用户拥有写权限的目录下才能创建。mkdir：不能创建目录tecmint，因为文件已经存在了。（上面的输出中不要被文件迷惑了，你应该记住我开头所说的-在linux中，文件，文件夹，驱动，命令，脚本都视为文件） 9. touch 命令“touch”命令代表了将文件的访问和修改时间更新为当前时间。touch命令只会在文件不存在的时候才会创建它。如果文件已经存在了，它会更新时间戳，但是并不会改变文件的内容。 root@tecmint:~``# touch tecmintfile 注意：touch 可以用来在用户拥有写权限的目录下创建不存在的文件。 10. chmod 命令“chmod”命令就是改变文件的模式位。chmod会根据要求的模式来改变每个所给的文件，文件夹，脚本等等的文件模式（权限）。 在文件(文件夹或者其它，为了简单起见，我们就使用文件)中存在3中类型的权限 Read (r)=4 Write(w)=2 Execute(x)=1 所以如果你想给文件只读权限，就设置为’4′;只写权限，设置权限为’2′;只执行权限，设置为1; 读写权限，就是4+2 = 6, 以此类推。 现在需要设置3种用户和用户组权限。第一个是拥有者，然后是用户所在的组，最后是其它用户。 rwxr-x–x abc.sh 这里root的权限是 rwx（读写和执行权限）， 所属用户组权限是 r-x (只有读写权限, 没有写权限)， 对于其它用户权限是 -x(只有只执行权限) 为了改变它的权限，为拥有者，用户所在组和其它用户提供读，写，执行权限。 root@tecmint:~``# chmod 777 abc.sh 三种都只有读写权限 root@tecmint:~``# chmod 666 abc.sh 拥有者用户有读写和执行权限，用户所在的组和其它用户只有可执行权限 root@tecmint:~``# chmod 711 abc.sh 注意：对于系统管理员和用户来说，这个命令是最有用的命令之一了。在多用户环境或者服务器上，对于某个用户，如果设置了文件不可访问，那么这个命令就可以解决，如果设置了错误的权限，那么也就提供了为授权的访问。 11. chown命令“chown”命令就是改变文件拥有者和所在用户组。每个文件都属于一个用户组和一个用户。在你的目录下，使用”ls -l”,你就会看到像这样的东西。 root@tecmint:~# ls -l drwxr-xr-x 3 server root 4096 May 10 11:14 Binarydrwxr-xr-x 2 server server 4096 May 13 09:42 Desktop在这里，目录Binary属于用户”server”,和用户组”root”,而目录”Desktop”属于用户“server”和用户组”server” “chown”命令用来改变文件的所有权，所以仅仅用来管理和提供文件的用户和用户组授权。 root@tecmint:~# chown server:server Binary drwxr-xr-x 3 server server 4096 May 10 11:14 Binarydrwxr-xr-x 2 server server 4096 May 13 09:42 Desktop注意：“chown”所给的文件改变用户和组的所有权到新的拥有者或者已经存在的用户或者用户组。 12. apt命令Debian系列以“apt”命令为基础，“apt”代表了Advanced Package Tool。APT是一个为Debian系列系统（Ubuntu，Kubuntu等等）开发的高级包管理器，在Gnu/Linux系统上，它会为包自动地， 智能地搜索，安装，升级以及解决依赖。 注意：上面的命令会导致系统整体的改变，所以需要root密码（查看提示符为”#”，而不是“$”）.和yum命令相比，Apt更高级和智能。 见名知义，apt-cache用来搜索包中是否包含子包mplayer, apt-get用来安装，升级所有的已安装的包到最新版。 关于apt-get 和 apt-cache命令更多信息，请查看 25 APT-GET和APT-CACHE命令 13. tar命令“tar”命令是磁带归档(Tape Archive)，对创建一些文件的的归档和它们的解压很有用。 root@tecmint:~# tar -zxvf abc.tar.gz (记住``&#39;z&#39;``代表了.tar.gz) root@tecmint:~# tar -jxvf abc.tar.bz2 (记住``&#39;j&#39;``代表了.tar.bz2) root@tecmint:~# tar -cvf archieve.tar.gz(.bz2) /path/to/folder/abc 注意： “tar.gz“代表了使用gzip归档，“bar.bz2”使用bzip压缩的，它压缩的更好但是也更慢。 了解更多”tar 命令”的例子，请查看 18 Tar命名例子 14. cal 命令“cal”（Calender），它用来显示当前月份或者未来或者过去任何年份中的月份。 root@tecmint:~# cal May 2013Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 10 1112 13 14 15 16 17 1819 20 21 22 23 24 2526 27 28 29 30 31&nbsp; 显示已经过去的月份，1835年2月 root@tecmint:~# cal 02 1835 February 1835Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 10 11 12 13 1415 16 17 18 19 20 2122 23 24 25 26 27 28显示未来的月份，2145年7月。 root@tecmint:~# cal 07 2145 July 2145 Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 1011 12 13 14 15 16 1718 19 20 21 22 23 2425 26 27 28 29 30 31&nbsp; 注意： 你不需要往回调整日历50年，既不用复杂的数据计算你出生那天，也不用计算你的生日在哪天到来，[因为它的最小单位是月，而不是日]。 15. date命令“date”命令使用标准的输出打印当前的日期和时间，也可以深入设置。 root@tecmint:~# date Fri May ``17 14``:``13``:``29 IST ``2013 root@tecmint:~# date --``set``=``&#39;14 may 2013 13:57&#39; Mon May ``13 13``:``57``:``00 IST ``2013 注意：这个命令在脚本中十分有用，以及基于时间和日期的脚本更完美。而且在终端中改变日期和时间，让你更专业！！！（当然你需要root权限才能操作这个，因为它是系统整体改变） 16. cat命令“cat”代表了连结（Concatenation），连接两个或者更多文本文件或者以标准输出形式打印文件的内容。 root@tecmint:~# cat a.txt b.txt c.txt d.txt abcd.txt root@tecmint:~# cat abcd.txt….contents of file abcd…注意：“&gt;&gt;”和“&gt;”调用了追加符号。它们用来追加到文件里，而不是显示在标准输出上。“&gt;”符号会删除已存在的文件，然后创建一个新的文件。所以因为安全的原因，建议使用“&gt;&gt;”，它会写入到文件中，而不是覆盖或者删除。 在深入探究之前，我必须让你知道通配符(你应该知道通配符，它出现在大多数电视选秀中)。通配符是shell的特色，和任何GUI文件管理器相比， 它使命令行更强大有力！如你所看到那样，在一个图形文件管理器中，你想选择一大组文件，你通常不得不使用你的鼠标来选择它们。这可能觉得很简单，但是事实 上，这种情形很让人沮丧！ 例如，假如你有一个有很多很多各种类型的文件和子目录的目录，然后你决定移动所有文件名中包含“Linux”字样的HTML文件到另外一个目录。如何简单的完成这个？如果目录中包含了大量的不同名的HTML文件，你的任务很巨大，而不是简单了。 在LInux CLI中，这个任务就很简单，就好像只移动一个HTML文件，因为有shell的通配符，才会如此简单。这些是特殊的字符，允许你选择匹配某种字符模式的 文件名。它帮助你来选择，即使是大量文件名中只有几个字符，而且在大多数情形中，它比使用鼠标选择文件更简单。 这里就是常用通配符列表： Wildcard Matches ``* 零个或者更多字符 ``? 恰好一个字符 [abcde] 恰好列举中的一个字符 ``[a-e] 恰好在所给范围中的一个字符 [!abcde] 任何字符都不在列举中 [!a-e] 任何字符都不在所给的范围中 {debian,linux} 恰好在所给选项中的一整个单词 ! 叫做非，带’！’的反向字符串为真 更多请阅读Linux cat 命令的实例 13 Linux中cat命令实例 17. cp 命令“copy”就是复制。它会从一个地方复制一个文件到另外一个地方。 root@tecmint:~# cp /home/user/Downloads abc.tar.gz /home/user/Desktop (Return ``0 when sucess) 注意： cp，在shell脚本中是最常用的一个命令，而且它可以使用通配符（在前面一块中有所描述），来定制所需的文件的复制。 18. mv 命令“mv”命令将一个地方的文件移动到另外一个地方去。 root@tecmint:~# mv /home/user/Downloads abc.tar.gz /home/user/Desktop (Return ``0 when sucess) 注意：mv 命令可以使用通配符。mv需谨慎使用，因为易懂系统的或者未授权的文件不但会导致安全性问题，而且可能系统崩溃。 19. pwd 命令“pwd”（print working directory），在终端中显示当前工作目录的全路径。 root@tecmint:~# pwd /home/user/Desktop 注意： 这个命令并不会在脚本中经常使用，但是对于新手，当从连接到nux很久后在终端中迷失了路径，这绝对是救命稻草。 20. cd 命令最后，经常使用的“cd”命令代表了改变目录。它在终端中改变工作目录来执行，复制，移动，读，写等等操作。 root@tecmint:~# cd /home/user/Desktop 注意： 在终端中切换目录时，cd就大显身手了。“cd ～”会改变工作目录为用户的家目录，而且当用户发现自己在终端中迷失了路径时，非常有用。“cd ..”从当前工作目录切换到(当前工作目录的)父目录。 这些命令肯定会让你在Linux上很舒服。但是这并不是结束。不久，我就会写一些其它的针对于中级用户的有用命令。例如，如果你熟练使用这些命令， 欢呼吧，少年，你会发现你已从小白级别提升为了中级用户了。在下篇文章，我会介绍像“kill”,”ps”,”grep”等等命令，期待吧，我不会让你失 望的。 // 参与翻译(2人)：Lesus, 赵亮-碧海情天]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WordPress-推荐插件]]></title>
    <url>%2F2013%2F07%2F11%2Fwordpress-plugins%2F</url>
    <content type="text"><![CDATA[WordPress用了有一段时间，感叹其插件功能的强大，对于很多新手或者像我这样对php代码不是很熟悉的人来说，确实节省了许多工作，这里简单介绍几个时下比较流行或者说个人比较推荐的插件： WordPress SEO The first true all-in-one SEO solution for WordPress, including on-page content analysis, XML sitemaps and much more.功能很强大的SEO（Search engine optimization），有中文化的版本，也有很方便的设置向导和引导，在发布文章时也可以进行SEO的分析，另外，内置了站点地图的功能，其他针对SEO的设置也很全面。 WP Super Cache Very fast caching plugin for WordPress.必备插件之一。是当前最高效也是最灵活的 WordPress 静态缓存插件。它把博客的网页直接生成 静态HTML 文件，就不用解析PHP脚本，从而使得你的 WordPress 博客显著提速。 W3 Total Cache 与WP Super Cache的功能比较类似，也是比较热门的一个插件，推荐使用。 备份插件 BackWPup 与 UpdraftPlus - Backup/Restore 正在使用后者， 计数统计 **WP-PostViews** WP-PostViews是一个文章计数统计插件，可以在文章中显示浏览数，还提供了一些统计功能，比如一定时间内浏览最多，评论最多等等，占用的系统资源也不多。 Google XML Sitemaps Google站点地图生成工具。 WP-PageNavi分页导航]]></content>
      <categories>
        <category>网站</category>
      </categories>
      <tags>
        <tag>wordpress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人博客小网站建设中]]></title>
    <url>%2F2013%2F07%2F11%2Funder-construction%2F</url>
    <content type="text"><![CDATA[This small blog website under construction]]></content>
      <categories>
        <category>日志</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PHP扩展相关]]></title>
    <url>%2F2013%2F07%2F01%2Fphp-extension%2F</url>
    <content type="text"><![CDATA[PHP扩展的介绍下面的内容大部分搜集与网络，或来自书籍的翻译，由我加以整理而成。 PHP取得成功的一个重要原因就是他拥有大量的可用扩展。Web开发者无论有何种需求，这种需求很有可能在PHP发行包里找到。PHP发行包包括支持各种数据库，图形文件格式，压缩，XML技术扩展在内的众多扩展。 有两个理由需要自己编写PHP扩展。第一个理由是：PHP需要支持一项她还未支持的技术。这通常包括包裹一些现成的C函数库，以便提供PHP接口。例如，如果一个叫FooBase的数据库已推出市场，你需要建立一个PHP扩展帮助你从PHP里调用FooBase的C函数库。这个工作可能仅由一个人完成，然后被整个PHP社区共享（如果你愿意的话）。第二个不是很普遍的理由是：你需要从性能或功能的原因考虑来编写一些商业逻辑。在PHP中构建扩展模块，主要有以下三种方式： External Modules：外部模块，也就是编译成共享库，用dl()函数动态加载。 好处：(1)不需要重新编译整个PHP(2)PHP体积小，因为不需要编译进PHP Built-in Modules：编译进PHP 好处：(1)不需要动态加载，模块在php脚本里面可以直接使用。(2)不需要将模块编译成.so共享库，因为直接编译进PHP。 缺点：(1)对模块的每次小变动都需要重新编译整个PHP和重新部署，不适于生产环节。(2)因为编译进PHP，所以PHP二进制文件较大，而且多占点内存. The Zend Engine：Zend核心里实现（参考Zend API） 关于dl函数可以参考下面链接：http://www.php.net/manual/zh/function.dl.php 可以不使用dl函数，而修改php.ini指定extension的加载，详细步骤如下： 进入PHP源代码包的ext目录，使用扩展骨架(skeleton)构造器， 为扩展建立函数的第一步是写一个函数定义文件 test.proto，该函数定义文件定义了扩展对外提供的函数原形。该例中，定义函数只有一行函数原形 int hello(int a, int b) 注意行后面没有分号 使用以下命令，生成对应的扩展目录test： ./ext_skel --extname=test --proto=test.proto 进入test目录，修改其中的config.m4文件 PHP_ARG_ENABLE(caleng_module, whether to enable test...[ --enable-test Enable test support])两行，删除前面的dnl。 修改test.c中的相关函数： PHP_FUNCTION(hello) 编译和安装模块 phpize./configure --with-php-config=/usr/local/php/bin/php-configmake &amp; make install 在PHP中启用扩展，修改配置文件/usr/local/php/etc/php.ini，加入 extension=test.so， 然后需要重启PHP或php-fpm。这里也可以通过在PHP代码中通过dl命令来调用该模块 测试扩展 &lt;?phpecho hello(1,2);phpinfo();?&gt;]]></content>
      <categories>
        <category>php</category>
      </categories>
      <tags>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jekyll bootstrap]]></title>
    <url>%2F2013%2F06%2F27%2Fjekyll-bootstrap%2F</url>
    <content type="text"><![CDATA[关于jekyll bootstrap详细的内容参考官方帮助文档 这里列出比较常用的命令： jekyll --server //在本地启动服务 rake post title=&quot;Hello World&quot; //新建一个小文章，标题为Hello World rake page name=&quot;about.md&quot; //新建一个页面about.md Octopress也是很不错的一个框架，有时间可以尝试一下。]]></content>
      <tags>
        <tag>jekyll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[制定计划-继续前行]]></title>
    <url>%2F2013%2F06%2F26%2Fhard-days%2F</url>
    <content type="text"><![CDATA[制定计划最近面临的问题很多，接下来要一个一个解决，直接列在下面的计划列表里面好了。 小论文的选题 毕设论文项目的进展 各个网站的发展目标和一些设想 暑假的时间安排 这几天的时间大部分都放在Web相关方面了，还是比较感兴趣的，学习了一些建站的内容，手里也搜集了几个域名，以后也许能用得上。也希望可以把Web的一些东西应用到毕设论文中去。 Github也逐渐用起来了，现在还很初步，还在学习的过程中。 另外，毕设项目没有什么特别的进展，Libevent的加入还是有许多困难，整体的改动还需要动很多代码，一直没敢下手，也没有特别好的思路，比较苦恼。 暑假也没有具体的安排，现在来看找实习基本是give up了，虽然挺不甘心的，但是时间上已经这样了，也没有办法，还是把时间安排到其他事情上去吧。]]></content>
      <categories>
        <category>Diary</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[我的时刻]]></title>
    <url>%2F2013%2F06%2F24%2Fmore-more%2F</url>
    <content type="text"><![CDATA[目前为止，为这个小型的博客网站绑定了两个域名，一个是lmshlms.com，是我之前经常用的用户名，一个是wodeshike.com（我的时刻），都可以用来访问！ 内容现在还比较少，以后再慢慢完善起来吧，敬请期待！]]></content>
      <categories>
        <category>日志</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[怪兽大学 Monster University]]></title>
    <url>%2F2013%2F06%2F24%2Fmonster-university%2F</url>
    <content type="text"><![CDATA[怪兽大学是今年很值得关注的一部电影，好莱坞在电影制作方面的水平确实一流，专门为怪兽大学制作了一个官方网站，这个不是介绍电影的网站，而是真正的大学网站哦！ 怪兽大学官网 &nbsp; &nbsp;]]></content>
      <categories>
        <category>日志</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux下的动态链接库和so文件]]></title>
    <url>%2F2013%2F06%2F24%2Flinux-library%2F</url>
    <content type="text"><![CDATA[Linux下动态链接库默认后缀是so，大部分的系统的动态链接库文件在以下目录：/lib /usr/lib /usr/local/libIf you ever happen to want to link against installed librariesin a given directory, LIBDIR, you must either use libtool, andspecify the full pathname of the library, or use the -LLIBDIR&#39; flag during linking and do at least one of the following: - add LIBDIR to theLD_LIBRARY_PATH’ environment variableduring execution- add LIBDIR to the LD_RUN_PATH&#39; environment variable during linking - use the-Wl,–rpath -Wl,LIBDIR’ linker flag- have your system administrator add LIBDIR to `/etc/ld.so.conf’ and run ldconfigSee any operating system documentation about shared libraries formore information, such as the ld(1) and ld.so(8) manual pages.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Amazon EC2学习笔记]]></title>
    <url>%2F2013%2F06%2F24%2Famazon-ec2-record%2F</url>
    <content type="text"><![CDATA[LNMP环境的安装http://www.giroro.com/amazon-ec2-study-notes-2/http://lnmp.org/install.html在EC2上架设Proxy或VPNhttp://hi.baidu.com/cnjimmydong/item/f61622ed08e7a3325a7cfbe2http://bleedfly.com/blog/ec2-openvpn.html到现在为止，Amazon EC2的使用已经暂告结束，其基本的一些使用与普通的VPS无异，但作为免费试用来说，还是要注意容易收费的小陷阱，毕竟Amazon EC2更多专注的是云计算平台，而不是普通的VPS！]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下安装配置LNMP--nginx+php+mysql]]></title>
    <url>%2F2013%2F06%2F24%2Fubuntu-lnmp%2F</url>
    <content type="text"><![CDATA[http://hi.baidu.com/jipiao_tejia/item/114bcf388c15398bf4e4ad75这里我们也可以使用LNMP一键安装包来搭建 Nginx+MySQL+PHP的环境，之后再在这个环境上面搭建包括WordPress在内的各种PHP应用。http://lnmp.org/install.html这里的安装过程叙述很详细，一步一步来就行，我在几个机器的Ubuntu12.04的环境下都是一次成功，所以还是比较推荐的，终端下直接运行命令 sudo ./ubuntu.sh即可。安装脚本确实做的非常不错，对各种依赖包都有考虑，configure的配置也很准确，可以正确编译各种源码，对于搭建LNMP的新手来说，确实很实用。对于爱学习的同学可以自行阅读理解一下各个安装的脚本文件，并以此作为进阶学习的基础。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[欢迎来到Github pages]]></title>
    <url>%2F2013%2F06%2F18%2Fmy-first-blog%2F</url>
    <content type="text"><![CDATA[标题1 This is a test of Markdown标题2 欢迎进入 Welcome标题3 Markdown标题4 这里是测试信息test info test test code code codecode here block hello not in order hello hello test code hello single asterisks single underscores double asterisks double underscores I get 10 times more traffic from Google than fromYahoo or MSN.]]></content>
      <tags>
        <tag>study</tag>
      </tags>
  </entry>
</search>
